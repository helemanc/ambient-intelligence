{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53cbfe96-445d-412c-a816-9fce99d3a0b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configuration\n",
    "\n",
    "NOTES: The warnings after the import are referred to the fact that Tensorflow 2.x versions are built to directly look for a GPU in the system. The warning can be forgot if you are not going to use the GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9f9cd086-654e-4b4f-9add-620a76f59989",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source myenv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "abe17403-93fd-44aa-b092-f164fcad2ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import IPython.display as ipd\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pickle\n",
    "import scipy\n",
    "import ipywidgets\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, AveragePooling1D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import SGD, Adam \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# from livelossplot import PlotLossesKeras\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2821a82d-85af-4802-b185-4df53d2e0c50",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351f1305-5c6b-4993-9dad-a91cb7e9934d",
   "metadata": {},
   "source": [
    "# Compute dataframes for datasets and split in Train, Val, Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5075b64c-04bc-4059-9f25-4ff31e0d6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = '/media/helemanc/OS/Users/i2CAT/Desktop/Datasets SER/'\n",
    "TESS = os.path.join(main_path, \"tess/TESS Toronto emotional speech set data/\") \n",
    "RAV = os.path.join(main_path, \"ravdess-emotional-speech-audio/audio_speech_actors_01-24\")\n",
    "SAVEE = os.path.join(main_path, \"savee/ALL/\")\n",
    "CREMA = os.path.join(main_path, \"creamd/AudioWAV/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "10092800-c0f7-41a4-839a-14f25f4afa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:00, 1083.71it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = []\n",
    "emotion = []\n",
    "voc_channel = []\n",
    "full_path = []\n",
    "modality = []\n",
    "intensity = []\n",
    "actors = []\n",
    "phrase =[]\n",
    "\n",
    "for root, dirs, files in tqdm(os.walk(RAV)):\n",
    "    for file in files:\n",
    "        try:\n",
    "            #Load librosa array, obtain mfcss, store the file and the mfcss information in a new array\n",
    "            # X, sample_rate = librosa.load(os.path.join(root,file), res_type='kaiser_fast')\n",
    "            # mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
    "            # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
    "            # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
    "           \n",
    "            modal = int(file[1:2])\n",
    "            vchan = int(file[4:5])\n",
    "            lab = int(file[7:8])\n",
    "            ints = int(file[10:11])\n",
    "            phr = int(file[13:14])\n",
    "            act = int(file[18:20])\n",
    "            # arr = mfccs, lab\n",
    "            # lst.append(arr)\n",
    "            \n",
    "            modality.append(modal)\n",
    "            voc_channel.append(vchan)\n",
    "            emotion.append(lab) #only labels\n",
    "            intensity.append(ints)\n",
    "            phrase.append(phr)\n",
    "            actors.append(act)\n",
    "            \n",
    "            full_path.append((root, file)) # only files\n",
    "          # If the file is not valid, skip it\n",
    "        except ValueError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "25baad03-4148-4b18-8869-b004d32bca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised\n",
    "# merge neutral and calm\n",
    "emotions_list = ['neutral', 'neutral', 'happy', 'sadness', 'angry', 'fear', 'disgust', 'surprise']\n",
    "emotion_dict = {em[0]+1:em[1] for em in enumerate(emotions_list)}\n",
    "\n",
    "df = pd.DataFrame([emotion, voc_channel, modality, intensity, actors, actors,phrase, full_path]).T\n",
    "df.columns = ['emotion', 'voc_channel', 'modality', 'intensity', 'actors', 'gender', 'phrase', 'path']\n",
    "df['emotion'] = df['emotion'].map(emotion_dict)\n",
    "df['voc_channel'] = df['voc_channel'].map({1: 'speech', 2:'song'})\n",
    "df['modality'] = df['modality'].map({1: 'full AV', 2:'video only', 3:'audio only'})\n",
    "df['intensity'] = df['intensity'].map({1: 'normal', 2:'strong'})\n",
    "df['actors'] = df['actors']\n",
    "df['gender'] = df['actors'].apply(lambda x: 'female' if x%2 == 0 else 'male')\n",
    "df['phrase'] = df['phrase'].map({1: 'Kids are talking by the door', 2:'Dogs are sitting by the door'})\n",
    "df['path'] = df['path'].apply(lambda x: x[0] + '/' + x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8f97f73b-1b33-4a29-a605-fe9e0afe5625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove files with noise to apply the same noise to all files for data augmentation \n",
    "df = df[~df.path.str.contains('noise')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dc2f69c3-ddaf-4238-a615-146815d3e3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only speech\n",
    "RAV_df = df\n",
    "RAV_df = RAV_df.loc[RAV_df.voc_channel == 'speech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5967b55f-7607-42e2-8552-06cebd816682",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAV_df.insert(0, \"emotion_label\", RAV_df.emotion, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "90eb0887-d825-4856-a25b-140337e5cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAV_df = RAV_df.drop(['emotion', 'voc_channel', 'modality', 'intensity', 'phrase'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "39986704-03ef-4e03-95f4-59e304913fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RAV_train = []\n",
    "RAV_val = []\n",
    "RAV_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "605bf138-949a-4405-9890-015702db5909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 120, 120)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in RAV_df.iterrows():\n",
    "    if row['actors'] in range(1,21): \n",
    "        RAV_train.append(row) \n",
    "    elif row['actors'] in range(21,23): \n",
    "        RAV_val.append(row)\n",
    "    elif row['actors'] in range(23,25): \n",
    "        RAV_test.append(row)\n",
    "len(RAV_train), len(RAV_val), len(RAV_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "067becd2-e407-4500-b9c7-9a9f3c0fc1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAV_train = pd.DataFrame(RAV_train)\n",
    "RAV_val = pd.DataFrame(RAV_val)\n",
    "RAV_test = pd.DataFrame(RAV_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "496fa08d-d067-4f62-aed2-532282f10e5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RAV_train = RAV_train.drop(['actors'], 1)\n",
    "RAV_val = RAV_val.drop(['actors'], 1)\n",
    "RAV_test = RAV_test.drop(['actors'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0eb180ed-6254-4503-99aa-97a334d5d52f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>gender</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>disgust</td>\n",
       "      <td>male</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disgust</td>\n",
       "      <td>male</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disgust</td>\n",
       "      <td>male</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disgust</td>\n",
       "      <td>male</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disgust</td>\n",
       "      <td>male</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emotion_label gender                                               path\n",
       "0       disgust   male  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...\n",
       "1       disgust   male  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...\n",
       "2       disgust   male  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...\n",
       "3       disgust   male  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...\n",
       "4       disgust   male  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset..."
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = RAV_train.reset_index(drop=True) \n",
    "df_val = RAV_val.reset_index(drop=True) \n",
    "df_test = RAV_test.reset_index(drop=True) \n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94e913b-ca3a-4bae-8dae-227380918988",
   "metadata": {},
   "source": [
    "# Create Noise Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "784ce1c9-8534-4bc5-bd8a-45a4923a28ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import random \n",
    "from pydub.utils import make_chunks\n",
    "\n",
    "\n",
    "def create_noise_files(df_train, df_val, df_test): \n",
    "    \n",
    "    '''\n",
    "    Apply noise only on training files, so double the number of training files and keep \n",
    "    validation and test the same\n",
    "    '''\n",
    "    path_noise_sound_1 = '/home/helemanc/Desktop/Binary_Model/noise_sounds/freight_train.wav'\n",
    "    path_noise_sound_2 = '/home/helemanc/Desktop/Binary_Model/noise_sounds/inside_train.wav'\n",
    "    path_noise_sound_3 = '/home/helemanc/Desktop/Binary_Model/noise_sounds/small_crowd.wav'\n",
    "    \n",
    "    path_noise_dataset_train = '/home/helemanc/Desktop/Binary_Model/noise_datasets/ravdess/train'\n",
    "    #path_noise_dataset_val = '/home/helemanc/Desktop/Binary_Model/noise_datasets/ravdess/val'\n",
    "    #path_noise_dataset_test = '/home/helemanc/Desktop/Binary_Model/noise_datasets/ravdess/test'\n",
    "    \n",
    "\n",
    "    #df_list = [df_train, df_val, df_test]\n",
    "    #count_df = 0 \n",
    "    \n",
    "    train_emotions = []\n",
    "    train_genders = []\n",
    "    train_paths = []\n",
    "    \n",
    "    #val_emotions = []\n",
    "    #val_genders = []\n",
    "    #val_paths = []\n",
    "    \n",
    "    #test_emotions = []\n",
    "    #test_genders = []\n",
    "    #test_paths = []\n",
    "    \n",
    "    #for df in df_list: \n",
    "        \n",
    "    for index, row in tqdm(df_train.iterrows()): \n",
    "        path = row['path']\n",
    "        sound1 = AudioSegment.from_file(path)\n",
    "        samples, sr = librosa.load(path, res_type='kaiser_fast', sr=16000)\n",
    "        duration = librosa.get_duration(y = samples, sr = sr)\n",
    "\n",
    "        # pick a noise sound file randomly \n",
    "        noise_list = [path_noise_sound_1, path_noise_sound_2, path_noise_sound_3]\n",
    "        random_noise = random.choice(noise_list) \n",
    "\n",
    "        lower_volume = 0 \n",
    "\n",
    "        # adjust volume to not cover the voice of the audio file \n",
    "        # warning: different levels of dB need to be calibrate for each dataset \n",
    "        '''\n",
    "        if random_noise == path_noise_sound_1: \n",
    "            lower_volume = 40\n",
    "        elif random_noise == path_noise_sound_2: \n",
    "            lower_volume = 25 \n",
    "        else: \n",
    "            lower_volume = 40\n",
    "        '''\n",
    "\n",
    "        # other strategy: \n",
    "        # compute db of both files, compute the difference, and lower the volume of the file to make it \n",
    "        # a bit lower than the original file -almost equal- \n",
    "\n",
    "        sound2 = AudioSegment.from_file(random_noise)\n",
    "\n",
    "        # make chunks of duration equal to the audio file \n",
    "        chunk_length_ms = duration*1000 #ms\n",
    "        chunks = make_chunks(sound2, chunk_length_ms) \n",
    "\n",
    "        # pick a random chunk \n",
    "        random_chunk = random.choice(chunks)\n",
    "        difference = random_chunk.dBFS - sound1.dBFS\n",
    "\n",
    "        abs_difference = abs(difference)\n",
    "\n",
    "        lower = random_chunk - abs_difference - 2\n",
    "\n",
    "        # lower the volume of the noise file to be overlayed with the voice_sound \n",
    "        #lower = random_chunk - lower_volume\n",
    "\n",
    "        combined = sound1.overlay(lower)\n",
    "\n",
    "        parts = path.split('/')\n",
    "        fname = parts[-1]\n",
    "        \n",
    "        new_path = path_noise_dataset_train + '/' + fname \n",
    "\n",
    "        train_emotions.append(row['emotion_label'])\n",
    "        train_genders.append(row['gender'])\n",
    "        train_paths.append(new_path)\n",
    "\n",
    "        '''\n",
    "        if count_df == 0: \n",
    "            new_path = path_noise_dataset_train + '/' + fname \n",
    "\n",
    "            train_emotions.append(row['emotion_label'])\n",
    "            train_genders.append(row['gender'])\n",
    "            train_paths.append(new_path)\n",
    "\n",
    "        elif count_df == 1: \n",
    "            new_path = path_noise_dataset_val + '/' + fname\n",
    "\n",
    "            val_emotions.append(row['emotion_label'])\n",
    "            val_genders.append(row['gender'])\n",
    "            val_paths.append(new_path)\n",
    "\n",
    "        elif count_df == 2:\n",
    "            new_path = path_noise_dataset_test + '/' + fname          \n",
    "\n",
    "            test_emotions.append(row['emotion_label'])\n",
    "            test_genders.append(row['gender'])\n",
    "            test_paths.append(new_path)\n",
    "        '''\n",
    "        combined.export(new_path, format= 'wav')\n",
    "\n",
    "    #count_df +=1\n",
    "\n",
    "    df_train_noise = pd.DataFrame([train_emotions, train_genders, train_paths]).T\n",
    "    df_train_noise.columns = ['emotion_label', 'gender', 'path']\n",
    "    \n",
    "    #df_val_noise = pd.DataFrame([val_emotions, val_genders, val_paths]).T\n",
    "    #df_val_noise.columns = ['emotion_label', 'gender', 'path']\n",
    "    \n",
    "    #df_test_noise = pd.DataFrame([test_emotions, test_genders, test_paths]).T\n",
    "    #df_test_noise.columns = ['emotion_label', 'gender', 'path']\n",
    "\n",
    "    df_train_combined = pd.concat([df_train, df_train_noise])\n",
    "    df_train_combined.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #df_val_combined = pd.concat([df_val, df_val_noise])\n",
    "    #df_val_combined.reset_index(drop=True, inplace=True)\n",
    "                                   \n",
    "    #df_test_combined = pd.concat([df_test, df_test_noise])\n",
    "    #df_test_combined.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df_train_combined, df_val, df_test\n",
    "# have to save df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ace55956-b4e7-4d15-a8ea-2cd85d87e5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1200it [00:04, 293.93it/s]\n"
     ]
    }
   ],
   "source": [
    "new_df_train, new_df_val, new_df_test = create_noise_files(df_train, df_val, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b3405ea7-5dee-4a5d-ba4b-93ac03d1560b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2400, 3), (120, 3), (120, 3))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_train.shape, new_df_val.shape, new_df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e270d2-7986-4fce-a63b-1de6a94d2d51",
   "metadata": {},
   "source": [
    "## Save dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "818574ac-d706-4cd3-9d9b-2abe96fb6bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_path = \"/home/helemanc/Desktop/Binary_Model/df_csv_noise/ravdess\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a0dcf71f-9df1-4b7c-a264-f0c1adb6a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_train.to_csv(os.path.join(preprocess_path,\"df_train.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cd99a021-c4d2-482b-97c2-3ae450260947",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df_val.to_csv(os.path.join(preprocess_path,\"df_val.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a65e6fa3-59e9-4c5e-98f6-a3b89808b3c3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df_test.to_csv(os.path.join(preprocess_path,\"df_test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3243cf61-dc2f-44de-8c1c-febffb9faeab",
   "metadata": {},
   "source": [
    "## Trial Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bd06393-9bd1-49cc-a9bc-1f4e45b5f6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_noise_sound_1 = '/home/helemanc/Desktop/Binary_Model/noise_sounds/freight_train.wav'\n",
    "path_noise_sound_2 = '/home/helemanc/Desktop/Binary_Model/noise_sounds/inside_train.wav'\n",
    "path_noise_sound_3 = '/home/helemanc/Desktop/Binary_Model/noise_sounds/small_crowd.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc4eeb6f-57bc-41af-94cc-fe7ab68caa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_noise_dataset = '/home/helemanc/Desktop/Binary_Model/noise_datasets/ravdess/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "81f7f167-8307-4e37-a05e-db50abcd7d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-23.519969873065545\n",
      "22.706629173010317\n",
      "-48.23292428007521\n",
      "-46.22659904607586\n"
     ]
    }
   ],
   "source": [
    "# load a file \n",
    "from pydub import AudioSegment\n",
    "import random \n",
    "from pydub.utils import make_chunks\n",
    "\n",
    "\n",
    "sound1 = AudioSegment.from_file(RAV_df.path[0])\n",
    "samples, sr = librosa.load(RAV_df.path[0], res_type='kaiser_fast', sr=16000)\n",
    "duration = librosa.get_duration(y = samples, sr = sr)\n",
    "                                \n",
    "noise_list = [path_noise_sound_1, path_noise_sound_2, path_noise_sound_3]\n",
    "lower_volume = 0 \n",
    "random_noise = random.choice(noise_list) \n",
    "'''\n",
    "if random_noise == path_noise_sound_1: \n",
    "    lower_volume = 30\n",
    "elif random_noise == path_noise_sound_2: \n",
    "    lower_volume = 25 \n",
    "else: \n",
    "    lower_volume = 40\n",
    "'''\n",
    "sound2 = AudioSegment.from_file(random_noise)\n",
    "\n",
    "chunk_length_ms = duration*1000 #ms\n",
    "chunks = make_chunks(sound2, chunk_length_ms) # divide the audio file to the original length\n",
    "\n",
    "random_chunk = random.choice(chunks)\n",
    "#lower = random_chunk - lower_volume\n",
    "print(random_chunk.dBFS)\n",
    "\n",
    "difference = random_chunk.dBFS - sound1.dBFS\n",
    "\n",
    "abs_difference = abs(difference)\n",
    "print(abs_difference)\n",
    "\n",
    "lower = random_chunk - abs_difference -2\n",
    "print(lower.dBFS)\n",
    "combined = sound1.overlay(lower)\n",
    "\n",
    "parts = RAV_df.path[0].split('/')\n",
    "fname = parts[-1]\n",
    "\n",
    "new_path = path_noise_dataset + '/' + fname \n",
    "\n",
    "combined.export(new_path, format= 'wav')\n",
    "print(sound1.dBFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8751e-8f2f-491e-b911-d951e934e7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6c0a2d-321e-42f7-862e-d2f075f8ba0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
