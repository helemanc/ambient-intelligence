{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b2cc517-86bd-45a3-bed3-3db51bfcd420",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configuration\n",
    "\n",
    "NOTES: The warnings after the import are referred to the fact that Tensorflow 2.x versions are built to directly look for a GPU in the system. The warning can be forgot if you are not going to use the GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d7b271-e749-4adf-b339-80aacb59832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source myenv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b0ab29f-d7e4-4e9c-9c51-793f82a7ea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples in 5 seconds of audio, 16 KHz sample rate \n",
    "LENGTH_CHOSEN =  80000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d12e92f-424c-4fc9-8187-1df15814d708",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-28 22:46:37.199153: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-28 22:46:37.199222: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-09-28 22:46:38.765982: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-28 22:46:38.769685: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-09-28 22:46:38.853775: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-09-28 22:46:38.853803: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (helemanc-Latitude-5410): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import IPython.display as ipd\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pickle\n",
    "import scipy\n",
    "import ipywidgets\n",
    "import math\n",
    "from time import time  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, AveragePooling1D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import SGD, Adam \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# from livelossplot import PlotLossesKeras\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae91dbd0-7e2e-434b-877f-17a5df198087",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e76108c-731a-4722-89bc-675e68f37db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_scaled(df_train, df_val, df_test): \n",
    "    load_train = load_files(df_train)\n",
    "    samples_train = extract_samples(load_train)\n",
    "    labels_train = extract_labels(df_train)\n",
    "    samples_train, labels_train = cut_and_pad(samples_train, labels_train)\n",
    "    samples_train, scaler = minmax_scaling_train(samples_train)\n",
    "    \n",
    "  \n",
    "    load_val = load_files(df_val)\n",
    "    samples_val = extract_samples(load_val)\n",
    "    labels_val = extract_labels(df_val)\n",
    "    samples_val, labels_val = cut_and_pad(samples_val, labels_val)\n",
    "    samples_val = np.array(samples_val)\n",
    "    samples_val = scaler.transform(samples_val)\n",
    "    labels_val = np.array(labels_val)\n",
    "    \n",
    "    \n",
    "    load_test = load_files(df_test)\n",
    "    samples_test = extract_samples(load_test)\n",
    "    labels_test = extract_labels(df_test)\n",
    "    samples_test, labels_test = cut_and_pad(samples_test, labels_test)\n",
    "    samples_test = np.array(samples_test)\n",
    "    \n",
    "    samples_test = scaler.transform(samples_test)\n",
    "    \n",
    "    labels_test = np.array(labels_test)\n",
    "\n",
    "    return samples_train, labels_train,  samples_val, labels_val, samples_test, labels_test\n",
    "\n",
    "def samples_scaled_tess(df_train, df_test): \n",
    "    load_train = load_files(df_train)\n",
    "    samples_train = extract_samples(load_train)\n",
    "    labels_train = extract_labels(df_train)\n",
    "    samples_train, labels_train = cut_and_pad(samples_train, labels_train)\n",
    "    samples_train, scaler = minmax_scaling_train(samples_train)\n",
    "    \n",
    "    \n",
    "    load_test = load_files(df_test)\n",
    "    samples_test = extract_samples(load_test)\n",
    "    labels_test = extract_labels(df_test)\n",
    "    samples_test, labels_test = cut_and_pad(samples_test, labels_test)\n",
    "    samples_test = np.array(samples_test)\n",
    "    \n",
    "    samples_test = scaler.transform(samples_test)\n",
    "    \n",
    "    labels_test = np.array(labels_test)\n",
    "\n",
    "    return samples_train, labels_train,  samples_test, labels_test\n",
    "\n",
    "\n",
    "def minmax_scaling_train(data):\n",
    "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    data = scaler.fit_transform(data)\n",
    "    return data, scaler\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72aadcc3-788d-460d-a4e6-9b2322969779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(df):\n",
    "    X = []\n",
    "    for i in tqdm(df['path']): \n",
    "        X.append(librosa.load(i, res_type='kaiser_fast', sr=16000))\n",
    "    return X\n",
    "\n",
    "def extract_samples(X): \n",
    "    samples = []\n",
    "    for ind,i in enumerate(X):\n",
    "        samples.append(i[0])\n",
    "    return samples \n",
    "\n",
    "def extract_labels(df): \n",
    "    labels = df['emotion_label'].copy()\n",
    "    return labels \n",
    "\n",
    "def compute_lengths(samples): \n",
    "    lengths = [len(x) for x in samples]\n",
    "    return lengths \n",
    "\n",
    "def check_outliers(lengths):\n",
    "    # outliers\n",
    "    lengths = np.array(lengths)\n",
    "    print((lengths > 300000).sum())\n",
    "    new_lengths = lengths[lengths < 300000]\n",
    "    return new_lengths \n",
    "\n",
    "def compute_mean_length(lengths): \n",
    "    return lengths.mean()\n",
    "\n",
    "\n",
    "def cut_and_pad(samples, labels, length_chosen = LENGTH_CHOSEN): \n",
    "    X_new = []\n",
    "    y_new = []\n",
    "    count = 0 \n",
    "    for ind,i in enumerate(samples):\n",
    "        if i.shape[0] < 300000:\n",
    "            if i.shape[0] > length_chosen:\n",
    "                new = i[:length_chosen]\n",
    "                X_new.append(new)\n",
    "            elif i.shape[0] < length_chosen:\n",
    "                new = np.pad(i,math.ceil((length_chosen-i.shape[0])/2), mode='median')\n",
    "                X_new.append(new)\n",
    "            else:\n",
    "                X_new.append(i)\n",
    "            y_new.append(labels[count])\n",
    "        count+=1\n",
    "    c = 0 \n",
    "    for el in X_new: \n",
    "        if len(el) == 80001: \n",
    "            X_new[c] = el[:-1]\n",
    "        c+=1\n",
    "    \n",
    "    return X_new, y_new\n",
    "    \n",
    "def compute_mfccs(samples, n_mfcc): \n",
    "    mfccs = []\n",
    "    for i in tqdm(samples):\n",
    "        mfcc = librosa.feature.mfcc(y=i, sr=16000, n_mfcc=n_mfcc)\n",
    "        mfcc = mfcc.T\n",
    "        mfcc = np.array(mfcc)\n",
    "        mfccs.append(mfcc[:, 1:]) # get rid of the first component \n",
    "    mfccs = np.array(mfccs)\n",
    "    return mfccs\n",
    "\n",
    "def compute_energy(samples): \n",
    "    energy_per_sample = []\n",
    "    for i in tqdm(samples):\n",
    "        energy = librosa.feature.rms(i)\n",
    "        energy = energy.T \n",
    "        energy = np.array(energy)\n",
    "        energy_per_sample.append(energy) \n",
    "    return energy_per_sample\n",
    "       \n",
    "def feature_extractor(df_train, df_val, df_test, n_mfcc): \n",
    "    samples_train, labels_train, samples_val, labels_val, samples_test, labels_test=samples_scaled(df_train, df_val, df_test)\n",
    "    mfccs_train = compute_mfccs(samples_train, n_mfcc = n_mfcc)\n",
    "    # energy \n",
    "    energy_train = compute_energy(samples_train) \n",
    "    features_train = []\n",
    "    for i in range(len(mfccs_train)): \n",
    "        if len(mfccs_train) == len(energy_train): \n",
    "            conc = np.concatenate((mfccs_train[i], energy_train[i]), axis = 1)\n",
    "            features_train.append(conc)\n",
    "\n",
    "    \n",
    "\n",
    "    mfccs_val = compute_mfccs(samples_val, n_mfcc = n_mfcc)\n",
    "    # energy \n",
    "    energy_val = compute_energy(samples_val) \n",
    "    features_val = []\n",
    "    for i in range(len(mfccs_val)): \n",
    "        if len(mfccs_val) == len(energy_val): \n",
    "            conc = np.concatenate((mfccs_val[i], energy_val[i]), axis = 1)\n",
    "            features_val.append(conc)\n",
    "    \n",
    "    \n",
    "    mfccs_test = compute_mfccs(samples_test, n_mfcc = n_mfcc)\n",
    "    # energy \n",
    "    energy_test = compute_energy(samples_test) \n",
    "    features_test=[]\n",
    "    for i in range(len(mfccs_test)): \n",
    "        if len(mfccs_test) == len(energy_test): \n",
    "            conc = np.concatenate((mfccs_test[i], energy_test[i]), axis = 1)\n",
    "            features_test.append(conc)\n",
    "    \n",
    "\n",
    "    return np.array(features_train), labels_train,  np.array(features_val), labels_val, np.array(features_test), labels_test\n",
    "    \n",
    "\n",
    "    \n",
    "def encode_labels(labels_train, labels_val, labels_test): \n",
    "    \n",
    "    emotion_enc = {'fear':1, 'disgust':1, 'neutral':0, 'calm':0,  'happy':0, 'sadness':1, 'surprise':0, 'angry':1}\n",
    "    y_train = pd.Series(labels_train).replace(emotion_enc)\n",
    "  \n",
    "    y_test = pd.Series(labels_test).map(emotion_enc)\n",
    "    y_val = pd.Series(labels_val).map(emotion_enc)\n",
    "    return y_train, y_val, y_test \n",
    "\n",
    "\n",
    "def encode_labels_tess(labels_train, labels_test): \n",
    "    \n",
    "    emotion_enc = {'fear':1, 'disgust':1, 'neutral':0, 'calm':0,  'happy':0, 'sadness':1, 'surprise':0, 'angry':1}\n",
    "    y_train = pd.Series(labels_train).replace(emotion_enc)\n",
    "  \n",
    "    y_test = pd.Series(labels_test).map(emotion_enc)\n",
    "    return y_train, y_test\n",
    "    \n",
    "def standard_scaling(X_train, X_val, X_test): \n",
    "  \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "    X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "    X_val = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
    "    return X_train, X_val, X_test \n",
    "    \n",
    "def standard_scaling_tess(X_train, X_test): \n",
    "  \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "    X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "    return X_train, X_test   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a8750-9007-46a9-b9f9-1e34764229c0",
   "metadata": {},
   "source": [
    "# Compute dataframes for datasets and split in Train, Val, Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "849e7bb6-03d6-47a3-a5ce-93b5baf0f1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = '/media/helemanc/OS/Users/i2CAT/Desktop/Datasets SER/'\n",
    "TESS = os.path.join(main_path, \"tess/TESS Toronto emotional speech set data/\") \n",
    "RAV = os.path.join(main_path, \"ravdess-emotional-speech-audio/audio_speech_actors_01-24\")\n",
    "SAVEE = os.path.join(main_path, \"savee/ALL/\")\n",
    "CREMA = os.path.join(main_path, \"creamd/AudioWAV/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a33717-5da3-4c05-83c0-356f25d918e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RADVESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9300b016-a956-423a-94e2-7ada6762eef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:00, 543.49it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = []\n",
    "emotion = []\n",
    "voc_channel = []\n",
    "full_path = []\n",
    "modality = []\n",
    "intensity = []\n",
    "actors = []\n",
    "phrase =[]\n",
    "\n",
    "for root, dirs, files in tqdm(os.walk(RAV)):\n",
    "    for file in files:\n",
    "        try:\n",
    "            #Load librosa array, obtain mfcss, store the file and the mfcss information in a new array\n",
    "            # X, sample_rate = librosa.load(os.path.join(root,file), res_type='kaiser_fast')\n",
    "            # mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
    "            # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
    "            # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
    "           \n",
    "            modal = int(file[1:2])\n",
    "            vchan = int(file[4:5])\n",
    "            lab = int(file[7:8])\n",
    "            ints = int(file[10:11])\n",
    "            phr = int(file[13:14])\n",
    "            act = int(file[18:20])\n",
    "            # arr = mfccs, lab\n",
    "            # lst.append(arr)\n",
    "            \n",
    "            modality.append(modal)\n",
    "            voc_channel.append(vchan)\n",
    "            emotion.append(lab) #only labels\n",
    "            intensity.append(ints)\n",
    "            phrase.append(phr)\n",
    "            actors.append(act)\n",
    "            \n",
    "            full_path.append((root, file)) # only files\n",
    "          # If the file is not valid, skip it\n",
    "        except ValueError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "810fc400-3c8a-4cab-b7c2-2d838845453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised\n",
    "# merge neutral and calm\n",
    "emotions_list = ['neutral', 'neutral', 'happy', 'sadness', 'angry', 'fear', 'disgust', 'surprise']\n",
    "emotion_dict = {em[0]+1:em[1] for em in enumerate(emotions_list)}\n",
    "\n",
    "df = pd.DataFrame([emotion, voc_channel, modality, intensity, actors, actors,phrase, full_path]).T\n",
    "df.columns = ['emotion', 'voc_channel', 'modality', 'intensity', 'actors', 'gender', 'phrase', 'path']\n",
    "df['emotion'] = df['emotion'].map(emotion_dict)\n",
    "df['voc_channel'] = df['voc_channel'].map({1: 'speech', 2:'song'})\n",
    "df['modality'] = df['modality'].map({1: 'full AV', 2:'video only', 3:'audio only'})\n",
    "df['intensity'] = df['intensity'].map({1: 'normal', 2:'strong'})\n",
    "df['actors'] = df['actors']\n",
    "df['gender'] = df['actors'].apply(lambda x: 'female' if x%2 == 0 else 'male')\n",
    "df['phrase'] = df['phrase'].map({1: 'Kids are talking by the door', 2:'Dogs are sitting by the door'})\n",
    "df['path'] = df['path'].apply(lambda x: x[0] + '/' + x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "624264a1-c7cb-4373-aa28-d7540ce8ed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove files with noise to apply the same noise to all files for data augmentation \n",
    "df = df[~df.path.str.contains('noise')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14b32cda-a4a8-4e98-8cc7-ae84ba23035b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>voc_channel</th>\n",
       "      <th>modality</th>\n",
       "      <th>intensity</th>\n",
       "      <th>actors</th>\n",
       "      <th>gender</th>\n",
       "      <th>phrase</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>disgust</td>\n",
       "      <td>speech</td>\n",
       "      <td>audio only</td>\n",
       "      <td>normal</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disgust</td>\n",
       "      <td>speech</td>\n",
       "      <td>audio only</td>\n",
       "      <td>strong</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>Kids are talking by the door</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disgust</td>\n",
       "      <td>speech</td>\n",
       "      <td>audio only</td>\n",
       "      <td>strong</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>Kids are talking by the door</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>disgust</td>\n",
       "      <td>speech</td>\n",
       "      <td>audio only</td>\n",
       "      <td>strong</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>disgust</td>\n",
       "      <td>speech</td>\n",
       "      <td>audio only</td>\n",
       "      <td>strong</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion voc_channel    modality intensity actors gender  \\\n",
       "0  disgust      speech  audio only    normal      1   male   \n",
       "2  disgust      speech  audio only    strong      1   male   \n",
       "4  disgust      speech  audio only    strong      1   male   \n",
       "6  disgust      speech  audio only    strong      1   male   \n",
       "8  disgust      speech  audio only    strong      1   male   \n",
       "\n",
       "                         phrase  \\\n",
       "0  Dogs are sitting by the door   \n",
       "2  Kids are talking by the door   \n",
       "4  Kids are talking by the door   \n",
       "6  Dogs are sitting by the door   \n",
       "8  Dogs are sitting by the door   \n",
       "\n",
       "                                                path  \n",
       "0  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "2  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "4  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "6  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "8  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0e5ccea-5790-4ffe-9c61-c8f6cdcd137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only speech\n",
    "RAV_df = df\n",
    "RAV_df = RAV_df.loc[RAV_df.voc_channel == 'speech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "badd6819-3750-40f0-8126-3ad08e993128",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAV_df.insert(0, \"emotion_label\", RAV_df.emotion, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b743eb49-edc5-4ff4-aa7b-713bd3ef3c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAV_df = RAV_df.drop(['emotion', 'voc_channel', 'modality', 'intensity', 'phrase'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1610de30-d139-4ff5-bd4d-9adf034a0478",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>actors</th>\n",
       "      <th>gender</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>disgust</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disgust</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disgust</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>disgust</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>disgust</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2871</th>\n",
       "      <td>neutral</td>\n",
       "      <td>24</td>\n",
       "      <td>female</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2873</th>\n",
       "      <td>neutral</td>\n",
       "      <td>24</td>\n",
       "      <td>female</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2875</th>\n",
       "      <td>neutral</td>\n",
       "      <td>24</td>\n",
       "      <td>female</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2877</th>\n",
       "      <td>neutral</td>\n",
       "      <td>24</td>\n",
       "      <td>female</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>neutral</td>\n",
       "      <td>24</td>\n",
       "      <td>female</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1440 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     emotion_label actors  gender  \\\n",
       "0          disgust      1    male   \n",
       "2          disgust      1    male   \n",
       "4          disgust      1    male   \n",
       "6          disgust      1    male   \n",
       "8          disgust      1    male   \n",
       "...            ...    ...     ...   \n",
       "2871       neutral     24  female   \n",
       "2873       neutral     24  female   \n",
       "2875       neutral     24  female   \n",
       "2877       neutral     24  female   \n",
       "2879       neutral     24  female   \n",
       "\n",
       "                                                   path  \n",
       "0     /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "2     /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "4     /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "6     /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "8     /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "...                                                 ...  \n",
       "2871  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "2873  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "2875  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "2877  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "2879  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "\n",
       "[1440 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAV_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83dc0d55-3df8-47fe-9a4f-feca731dd328",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RAV_train = []\n",
    "RAV_val = []\n",
    "RAV_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1717febc-c1a7-4a3e-83d5-83c9208531c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 120, 120)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in RAV_df.iterrows():\n",
    "    if row['actors'] in range(1,21): \n",
    "        RAV_train.append(row) \n",
    "    elif row['actors'] in range(21,23): \n",
    "        RAV_val.append(row)\n",
    "    elif row['actors'] in range(23,25): \n",
    "        RAV_test.append(row)\n",
    "len(RAV_train), len(RAV_val), len(RAV_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "415c0af8-af44-48a5-8c45-0708e9aa0866",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAV_train = pd.DataFrame(RAV_train)\n",
    "RAV_val = pd.DataFrame(RAV_val)\n",
    "RAV_test = pd.DataFrame(RAV_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9933e6eb-dbf2-4e0e-a18e-271c05f5634a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RAV_train = RAV_train.drop(['actors'], 1)\n",
    "RAV_val = RAV_val.drop(['actors'], 1)\n",
    "RAV_test = RAV_test.drop(['actors'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49947dbf-32bc-411c-a389-f9ff0ea5cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAV_train.reset_index(drop=True, inplace = True) \n",
    "RAV_val.reset_index(drop=True, inplace = True) \n",
    "RAV_test.reset_index(drop=True, inplace = True ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92002637-88a2-4029-8190-822793bced9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SAVEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "368898a8-d8f4-4cc7-b12c-e7c68ba8e6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     120\n",
       "sadness      60\n",
       "surprise     60\n",
       "happy        60\n",
       "disgust      60\n",
       "fear         60\n",
       "angry        60\n",
       "Name: emotion_label, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the data location for SAVEE\n",
    "dir_list = os.listdir(SAVEE)\n",
    "\n",
    "# parse the filename to get the emotions\n",
    "emotion=[]\n",
    "path = []\n",
    "actors = []\n",
    "gender = []\n",
    "for i in dir_list:\n",
    "    actors.append(i[:2])\n",
    "    if i[-8:-6]=='_a':\n",
    "        emotion.append('angry')\n",
    "        gender.append('male')\n",
    "    elif i[-8:-6]=='_d':\n",
    "        emotion.append('disgust')\n",
    "        gender.append('male')\n",
    "    elif i[-8:-6]=='_f':\n",
    "        emotion.append('fear')\n",
    "        gender.append('male')\n",
    "    elif i[-8:-6]=='_h':\n",
    "        emotion.append('happy')\n",
    "        gender.append('male')\n",
    "    elif i[-8:-6]=='_n':\n",
    "        emotion.append('neutral')\n",
    "        gender.append('male')\n",
    "    elif i[-8:-6]=='sa':\n",
    "        emotion.append('sadness')\n",
    "        gender.append('male')\n",
    "    elif i[-8:-6]=='su':\n",
    "        emotion.append('surprise')\n",
    "        gender.append('male') \n",
    "    else:\n",
    "        emotion.append('Unknown') \n",
    "    path.append(SAVEE + i)\n",
    "    \n",
    "# Now check out the label count distribution \n",
    "SAVEE_df = pd.DataFrame(emotion, columns = ['emotion_label'])\n",
    "                      \n",
    "SAVEE_df = pd.concat([SAVEE_df,\n",
    "                      pd.DataFrame(actors, columns = ['actors']),\n",
    "                      pd.DataFrame(gender, columns = ['gender']), \n",
    "                      pd.DataFrame(path, columns = ['path'])], axis = 1)\n",
    "SAVEE_df.emotion_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe6f05ee-a0d2-455a-b41b-1b2d4742b98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>actors</th>\n",
       "      <th>gender</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>DC</td>\n",
       "      <td>male</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>KL</td>\n",
       "      <td>male</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>KL</td>\n",
       "      <td>male</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sadness</td>\n",
       "      <td>KL</td>\n",
       "      <td>male</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sadness</td>\n",
       "      <td>KL</td>\n",
       "      <td>male</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emotion_label actors gender  \\\n",
       "0       neutral     DC   male   \n",
       "1       sadness     KL   male   \n",
       "2       sadness     KL   male   \n",
       "3       sadness     KL   male   \n",
       "4       sadness     KL   male   \n",
       "\n",
       "                                                path  \n",
       "0  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "1  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "2  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "3  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "4  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVEE_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4da8df8f-e515-4e06-a8ea-e701112bb18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEE_train = []\n",
    "SAVEE_val = []\n",
    "SAVEE_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22d9210a-1f62-44b5-9772-f93f033f8b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 120, 120)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DC, JE, JK, KL\n",
    "for index, row in SAVEE_df.iterrows(): \n",
    "    if row['actors'] == 'DC' or row ['actors'] == 'JE':\n",
    "        SAVEE_train.append(row)\n",
    "    elif row['actors'] == 'JK': \n",
    "        SAVEE_val.append(row)\n",
    "    else: \n",
    "        SAVEE_test.append(row)\n",
    "len(SAVEE_train), len(SAVEE_val), len(SAVEE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "433acf97-fc80-440c-9a81-7784dd719269",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEE_train = pd.DataFrame(SAVEE_train)\n",
    "SAVEE_val = pd.DataFrame(SAVEE_val)\n",
    "SAVEE_test = pd.DataFrame(SAVEE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bf7befd-5978-460d-8352-d5dc87ec138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEE_train = SAVEE_train.drop(['actors'], 1)\n",
    "SAVEE_val = SAVEE_val.drop(['actors'], 1)\n",
    "SAVEE_test = SAVEE_test.drop(['actors'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75ee63fa-9bf7-4dc1-b53d-2ba7178a9d78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAVEE_train = SAVEE_train.reset_index(drop=True) \n",
    "SAVEE_val = SAVEE_val.reset_index(drop=True) \n",
    "SAVEE_test = SAVEE_test.reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33794eb-89fe-44bf-8e63-22a2daefadc0",
   "metadata": {},
   "source": [
    "## TESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba7cd08f-ac9c-472c-98ec-7d8664df08d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "angry       1200\n",
       "fear         800\n",
       "surprise     800\n",
       "sadness      800\n",
       "disgust      800\n",
       "neutral      800\n",
       "happy        400\n",
       "Name: emotion_label, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_list = os.listdir(TESS)\n",
    "dir_list.sort()\n",
    "dir_list\n",
    "\n",
    "path = []\n",
    "emotion = []\n",
    "gender = []\n",
    "actors = []\n",
    "\n",
    "for i in dir_list:\n",
    "    fname = os.listdir(TESS + i)\n",
    "    for f in fname:\n",
    "        if i == 'OAF_angry':\n",
    "            emotion.append('angry')\n",
    "            gender.append('female')\n",
    "            actors.append('OAF')\n",
    "        elif i == 'YAF_angry': \n",
    "            emotion.append('angry')\n",
    "            gender.append('female')\n",
    "            actors.append('YAF')\n",
    "            \n",
    "            \n",
    "        elif i == 'OAF_disgust' :\n",
    "            emotion.append('disgust')\n",
    "            gender.append('female')\n",
    "            actors.append('OAF')\n",
    "        elif i == 'YAF_disgust': \n",
    "            emotion.append('disgust')\n",
    "            gender.append('female')\n",
    "            actors.append('YAF')\n",
    "            \n",
    "            \n",
    "        elif i == 'OAF_Fear':\n",
    "            emotion.append('fear')\n",
    "            gender.append('female')\n",
    "            actors.append('OAF')\n",
    "        elif i == 'YAF_fear': \n",
    "            emotion.append('fear')\n",
    "            gender.append('female')\n",
    "            actors.append('YAF') \n",
    "            \n",
    "            \n",
    "        elif i == 'OAF_happy' :\n",
    "            emotion.append('happy')\n",
    "            gender.append('female')\n",
    "            actors.append('OAF')\n",
    "        elif i == 'YAF_happy': \n",
    "            emotion.append('angry')\n",
    "            gender.append('female')\n",
    "            actors.append('YAF')            \n",
    "            \n",
    "        elif i == 'OAF_neutral':\n",
    "            emotion.append('neutral')\n",
    "            gender.append('female')\n",
    "            actors.append('OAF')   \n",
    "        elif i == 'YAF_neutral': \n",
    "            emotion.append('neutral')\n",
    "            gender.append('female')\n",
    "            actors.append('YAF')      \n",
    "            \n",
    "                \n",
    "        elif i == 'OAF_Pleasant_surprise':\n",
    "            emotion.append('surprise')\n",
    "            gender.append('female')\n",
    "            actors.append('OAF')\n",
    "        \n",
    "        elif i == 'YAF_pleasant_surprised': \n",
    "            emotion.append('surprise')\n",
    "            gender.append('female')\n",
    "            actors.append('YAF')            \n",
    "            \n",
    "        elif i == 'OAF_Sad':\n",
    "            emotion.append('sadness')\n",
    "            gender.append('female')\n",
    "            actors.append('OAF')\n",
    "        elif i == 'YAF_sad': \n",
    "            emotion.append('sadness')\n",
    "            gender.append('female')\n",
    "            actors.append('YAF')            \n",
    "        else:\n",
    "            emotion.append('Unknown')\n",
    "        path.append(TESS + i + \"/\" + f)\n",
    "\n",
    "TESS_df = pd.DataFrame(emotion, columns = ['emotion_label'])\n",
    "TESS_df = pd.concat([TESS_df, pd.DataFrame(gender, columns = ['gender']), \n",
    "                     pd.DataFrame(actors, columns= ['actors']),\n",
    "                     pd.DataFrame(path, columns = ['path'])],axis=1)\n",
    "TESS_df.emotion_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73b5989e-545c-4a80-a91e-be4bc2a6fda6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TESS_df= TESS_df[~TESS_df.path.str.contains('noise')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36993eca-1a86-4764-9ce2-6cd10006fe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESS_train = []\n",
    "TESS_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5916dcb9-820c-4ec9-a581-a578899f452f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 1400)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in TESS_df.iterrows(): \n",
    "    if row['actors'] == 'YAF': \n",
    "        TESS_train.append(row)\n",
    "    else: \n",
    "        TESS_test.append(row)\n",
    "len(TESS_train), len(TESS_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c46e409b-67bb-4089-a6ee-7ea2314ad6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESS_train = pd.DataFrame(TESS_train)\n",
    "TESS_test = pd.DataFrame(TESS_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1f9780e-8c39-4d02-a506-b54f12670f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESS_train = TESS_train.reset_index(drop=True) \n",
    "TESS_test  = TESS_test.reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecde17f-475d-479c-800b-5275974f0d4c",
   "metadata": {},
   "source": [
    "## CREMA-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f463a79-0b3b-4108-8112-1d8b9f29c9da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "males = [1,\n",
    "5,\n",
    "11,\n",
    "14,\n",
    "15,\n",
    "16,\n",
    "17,\n",
    "19,\n",
    "22,\n",
    "23,\n",
    "26,\n",
    "27,\n",
    "31,\n",
    "32,\n",
    "33,\n",
    "34,\n",
    "35,\n",
    "36,\n",
    "38,\n",
    "39,\n",
    "41,\n",
    "42,\n",
    "44,\n",
    "45,\n",
    "48,\n",
    "50,\n",
    "51,\n",
    "57,\n",
    "59, \n",
    "62, \n",
    "64,\n",
    "65, \n",
    "66,\n",
    "67,\n",
    "68,\n",
    "69,\n",
    "70,\n",
    "71,\n",
    "77, \n",
    "80, \n",
    "81, \n",
    "83, \n",
    "85, \n",
    "86, \n",
    "87,\n",
    "88, \n",
    "90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7355980-8ff4-4447-b71b-423401a26a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "females = [ 2,\n",
    "3,\n",
    "4,\n",
    "6,\n",
    "7,\n",
    "8,\n",
    "9,\n",
    "10,\n",
    "12,\n",
    "13,\n",
    "18,\n",
    "20,\n",
    "21,\n",
    "24,\n",
    "25,\n",
    "28,\n",
    "29,\n",
    "30,\n",
    "37,\n",
    "40,\n",
    "43,\n",
    "46,\n",
    "47,\n",
    "49,\n",
    "52,\n",
    "53,\n",
    "54,\n",
    "55,\n",
    "56, \n",
    "58, \n",
    "60,\n",
    "61,\n",
    "63,\n",
    "72, \n",
    "73, \n",
    "74, \n",
    "75, \n",
    "76, \n",
    "78, \n",
    "79, \n",
    "82, \n",
    "84, \n",
    "89, \n",
    "91]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9b58b82-d6a3-4b51-bae3-72905639a02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>actors</th>\n",
       "      <th>gender</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>happy</td>\n",
       "      <td>91</td>\n",
       "      <td>female</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>91</td>\n",
       "      <td>female</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>angry</td>\n",
       "      <td>91</td>\n",
       "      <td>female</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disgust</td>\n",
       "      <td>91</td>\n",
       "      <td>female</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fear</td>\n",
       "      <td>91</td>\n",
       "      <td>female</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emotion_label actors  gender  \\\n",
       "0         happy     91  female   \n",
       "1       sadness     91  female   \n",
       "2         angry     91  female   \n",
       "3       disgust     91  female   \n",
       "4          fear     91  female   \n",
       "\n",
       "                                                path  \n",
       "0  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "1  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "2  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "3  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "4  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crema_directory_list = os.listdir(CREMA)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "actors = []\n",
    "gender = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for file in crema_directory_list:\n",
    "\n",
    "    # storing file emotions\n",
    "    part=file.split('_')\n",
    "    \n",
    "    # use only high intensity files\n",
    "    if \"HI\" in part[3] :\n",
    "        actor = part[0][2:]\n",
    "        actors.append(actor)\n",
    "        if int(actor) in males:\n",
    "            gender.append('male')\n",
    "        else: \n",
    "            gender.append('female')\n",
    "    \n",
    "        # storing file paths\n",
    "        file_path.append(CREMA + file)\n",
    "        if part[2] == 'SAD':\n",
    "            file_emotion.append('sadness')\n",
    "        elif part[2] == 'ANG':\n",
    "            file_emotion.append('angry')\n",
    "        elif part[2] == 'DIS':\n",
    "            file_emotion.append('disgust')\n",
    "        elif part[2] == 'FEA':\n",
    "            file_emotion.append('fear')\n",
    "        elif part[2] == 'HAP':\n",
    "            file_emotion.append('happy')\n",
    "        elif part[2] == 'NEU':\n",
    "            file_emotion.append('neutral')\n",
    "        else:\n",
    "            file_emotion.append('Unknown')\n",
    "\n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['emotion_label'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['path'])\n",
    "actors_df = pd.DataFrame(actors, columns=['actors'])\n",
    "gender_df = pd.DataFrame(gender, columns=['gender'])                      \n",
    "Crema_df = pd.concat([emotion_df, actors_df, gender_df, path_df], axis=1)\n",
    "Crema_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3522c22-6c0b-48e5-bdf5-4a0e65dd3266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 4)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Crema_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d796f3f-3e17-40fc-a414-4c6e882098ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_files = {}\n",
    "\n",
    "for index, row in Crema_df.iterrows():\n",
    "    actor = row['actors']\n",
    "    if actor not in actor_files.keys(): \n",
    "        actor_files[actor] = 1\n",
    "    else: \n",
    "        actor_files[actor]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc3ebd86-6c77-4ab7-bac8-a751977ee32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'91': 5,\n",
       " '90': 5,\n",
       " '89': 5,\n",
       " '88': 5,\n",
       " '87': 5,\n",
       " '86': 5,\n",
       " '85': 5,\n",
       " '84': 5,\n",
       " '83': 5,\n",
       " '82': 5,\n",
       " '81': 5,\n",
       " '80': 5,\n",
       " '79': 5,\n",
       " '78': 5,\n",
       " '77': 5,\n",
       " '76': 5,\n",
       " '75': 5,\n",
       " '74': 5,\n",
       " '73': 5,\n",
       " '72': 5,\n",
       " '71': 5,\n",
       " '70': 5,\n",
       " '69': 5,\n",
       " '68': 5,\n",
       " '67': 5,\n",
       " '66': 5,\n",
       " '65': 5,\n",
       " '64': 5,\n",
       " '63': 5,\n",
       " '62': 5,\n",
       " '61': 5,\n",
       " '60': 5,\n",
       " '59': 5,\n",
       " '58': 5,\n",
       " '57': 5,\n",
       " '56': 5,\n",
       " '55': 5,\n",
       " '54': 5,\n",
       " '53': 5,\n",
       " '52': 5,\n",
       " '51': 5,\n",
       " '50': 5,\n",
       " '49': 5,\n",
       " '48': 5,\n",
       " '47': 5,\n",
       " '46': 5,\n",
       " '45': 5,\n",
       " '44': 5,\n",
       " '43': 5,\n",
       " '42': 5,\n",
       " '41': 5,\n",
       " '40': 5,\n",
       " '39': 5,\n",
       " '38': 5,\n",
       " '37': 5,\n",
       " '36': 5,\n",
       " '35': 5,\n",
       " '34': 5,\n",
       " '33': 5,\n",
       " '32': 5,\n",
       " '31': 5,\n",
       " '30': 5,\n",
       " '29': 5,\n",
       " '28': 5,\n",
       " '27': 5,\n",
       " '26': 5,\n",
       " '25': 5,\n",
       " '24': 5,\n",
       " '23': 5,\n",
       " '22': 5,\n",
       " '21': 5,\n",
       " '20': 5,\n",
       " '19': 5,\n",
       " '18': 5,\n",
       " '17': 5,\n",
       " '16': 5,\n",
       " '15': 5,\n",
       " '14': 5,\n",
       " '13': 5,\n",
       " '12': 5,\n",
       " '11': 5,\n",
       " '10': 5,\n",
       " '09': 5,\n",
       " '08': 5,\n",
       " '07': 5,\n",
       " '06': 5,\n",
       " '05': 5,\n",
       " '04': 5,\n",
       " '03': 5,\n",
       " '02': 5,\n",
       " '01': 5}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ccd0e8f-1ca1-4ad0-a6da-1f5affc757cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_males = 0 \n",
    "count_females = 0 \n",
    "male_list = []\n",
    "for index, row in Crema_df.iterrows(): \n",
    "    gender = row['gender']\n",
    "    actor = row['actors']\n",
    "    if gender == 'male':\n",
    "        count_males +=1\n",
    "        if actor not in male_list: \n",
    "            male_list.append(actor)\n",
    "    else: \n",
    "        count_females +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "adcd7330-6fc9-4176-8837-b4bcbb707a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 220)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_males, count_females"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87d1d77-2fbe-4b4e-9d89-2e40a615a8a6",
   "metadata": {},
   "source": [
    "Since there are more males than females we will remove randomly 3 male actors (since there are exactly 5 audio files per actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "901f8666-3d77-4833-acdc-b1080d346d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['17', '80', '88']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "random.seed(42)\n",
    "males_to_remove = random.sample(male_list, 3)\n",
    "males_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45dcbde7-05bc-4b7f-9ed4-f22ffa800a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = []\n",
    "for index, row in Crema_df.iterrows(): \n",
    "    if row['actors'] not in males_to_remove: \n",
    "        new_df.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a0893b2-d435-4edd-a7de-0312db92adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREMA_df = pd.DataFrame(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0722d1fe-94de-41e0-979e-cb4c7a2d331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in CREMA_df.iterrows(): \n",
    "    if row['actors'] == '17': \n",
    "        print(\"Elements not removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd0a40e1-2c22-466c-82f6-acef85db3554",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_males = 0 \n",
    "count_females = 0 \n",
    "male_list = []\n",
    "female_list = []\n",
    "for index, row in CREMA_df.iterrows(): \n",
    "    gender = row['gender']\n",
    "    actor = row['actors']\n",
    "    if gender == 'male':\n",
    "        count_males +=1\n",
    "        if actor not in male_list: \n",
    "            male_list.append(actor)\n",
    "    else: \n",
    "        count_females +=1\n",
    "        if actor not in female_list: \n",
    "            female_list.append(actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "97903455-740f-4f02-b5af-38be0dd1691f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 220)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_males, count_females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a29f43e-380a-42a3-ae3d-168f41eabd35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(female_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9b023e3-07dc-41d6-b74c-dc6f4ad3478e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(male_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12cf961a-6853-4902-ad22-0f9666fc2bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREMA_train = []\n",
    "CREMA_val = []\n",
    "CREMA_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fa152a80-3ef3-415e-9e57-6ee99081ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "females_train = random.sample(female_list, 32)\n",
    "males_train = random.sample(male_list, 32)\n",
    "\n",
    "# remove the elements assigned to train \n",
    "for element in females_train:\n",
    "    if element in female_list:\n",
    "        female_list.remove(element)\n",
    "        \n",
    "for element in males_train:\n",
    "    if element in male_list:\n",
    "        male_list.remove(element)\n",
    "\n",
    "         \n",
    "females_val = random.sample(female_list, 6) \n",
    "males_val = random.sample(male_list, 6) \n",
    "\n",
    "# remove the elements assigned to val\n",
    "for element in females_val:\n",
    "    if element in female_list:\n",
    "        female_list.remove(element)\n",
    "        \n",
    "for element in males_val:\n",
    "    if element in male_list:\n",
    "        male_list.remove(element)\n",
    "        \n",
    "females_test = random.sample(female_list, 6) \n",
    "males_test = random.sample(male_list, 6)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e842b9a1-d8e3-482a-9efb-6c4365f4fbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['54',\n",
       "  '56',\n",
       "  '58',\n",
       "  '74',\n",
       "  '76',\n",
       "  '13',\n",
       "  '78',\n",
       "  '29',\n",
       "  '84',\n",
       "  '89',\n",
       "  '09',\n",
       "  '60',\n",
       "  '04',\n",
       "  '55',\n",
       "  '52',\n",
       "  '91',\n",
       "  '02',\n",
       "  '07',\n",
       "  '46',\n",
       "  '49',\n",
       "  '37',\n",
       "  '10',\n",
       "  '20',\n",
       "  '75',\n",
       "  '21',\n",
       "  '53',\n",
       "  '06',\n",
       "  '28',\n",
       "  '18',\n",
       "  '63',\n",
       "  '30',\n",
       "  '03'],\n",
       " ['57',\n",
       "  '69',\n",
       "  '65',\n",
       "  '45',\n",
       "  '77',\n",
       "  '81',\n",
       "  '41',\n",
       "  '15',\n",
       "  '44',\n",
       "  '23',\n",
       "  '59',\n",
       "  '86',\n",
       "  '34',\n",
       "  '01',\n",
       "  '85',\n",
       "  '66',\n",
       "  '31',\n",
       "  '33',\n",
       "  '05',\n",
       "  '48',\n",
       "  '50',\n",
       "  '67',\n",
       "  '51',\n",
       "  '22',\n",
       "  '36',\n",
       "  '87',\n",
       "  '71',\n",
       "  '39',\n",
       "  '42',\n",
       "  '11',\n",
       "  '32',\n",
       "  '14'],\n",
       " ['43', '61', '40', '47', '73', '24'],\n",
       " ['62', '68', '64', '83', '70', '26'],\n",
       " ['08', '79', '12', '25', '72', '82'],\n",
       " ['16', '19', '38', '35', '27', '90'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "females_train, males_train, females_val, males_val, females_test, males_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4664fa03-d2fd-4c2d-9aa2-9aa0ab8cae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = females_train + males_train \n",
    "val = females_val + males_val \n",
    "test = females_test + males_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8a4f4990-eb24-4e0e-a005-e57523552de8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index, row in CREMA_df.iterrows(): \n",
    "    gender = row['gender']\n",
    "    actor = row['actors']\n",
    "    if actor in train: \n",
    "        CREMA_train.append(row)\n",
    "    elif actor in val: \n",
    "        CREMA_val.append(row)\n",
    "    else:\n",
    "        CREMA_test.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fda8cdc4-8e3e-4920-9356-4a1df803ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREMA_train = pd.DataFrame(CREMA_train) \n",
    "CREMA_val = pd.DataFrame(CREMA_val) \n",
    "CREMA_test = pd.DataFrame(CREMA_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c6cc9a30-4062-4fcb-babf-9533951becf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((320, 4), (60, 4), (60, 4))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CREMA_train.shape, CREMA_val.shape, CREMA_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5ec9bad8-b437-4685-89ae-8973a37dcb34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CREMA_train = CREMA_train.reset_index(drop=True) \n",
    "CREMA_val = CREMA_val.reset_index(drop = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9a0096-52e1-4551-b379-3f9c7eba7986",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiment 8.10: RAVDESS - TESS - SAVEE noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2214b1fe-9fd0-41e7-b8a6-6122d8767019",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Read dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f858c34-5271-47bb-b44a-7bf62f574041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocess_path_rav = \"/home/helemanc/Desktop/Binary_Model/df_csv_noise/ravdess\"\n",
    "preprocess_path_savee = \"/home/helemanc/Desktop/Binary_Model/df_csv_noise/savee\"\n",
    "preprocess_path_tess = \"/home/helemanc/Desktop/Binary_Model/df_csv_noise/tess\"\n",
    "\n",
    "df_train_rav = pd.read_csv(os.path.join(preprocess_path_rav,\"df_train.csv\"))\n",
    "df_val_rav = pd.read_csv(os.path.join(preprocess_path_rav,\"df_val.csv\"))\n",
    "df_test_rav = pd.read_csv(os.path.join(preprocess_path_rav,\"df_test.csv\"))  \n",
    "\n",
    "df_train_tess = pd.read_csv(os.path.join(preprocess_path_tess,\"df_train.csv\"))\n",
    "df_test_tess= pd.read_csv(os.path.join(preprocess_path_tess,\"df_test.csv\"))  \n",
    "\n",
    "df_train_savee = pd.read_csv(os.path.join(preprocess_path_savee,\"df_train.csv\"))\n",
    "df_val_savee = pd.read_csv(os.path.join(preprocess_path_savee,\"df_val.csv\"))\n",
    "df_test_savee = pd.read_csv(os.path.join(preprocess_path_savee,\"df_test.csv\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "abd2f1fb-ed60-4ce9-9698-657a9cc9ac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train_rav, df_train_savee, df_train_tess])\n",
    "df_val = pd.concat([df_val_rav, df_val_savee])\n",
    "df_test = pd.concat([df_test_rav, df_test_savee, df_test_tess])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0987a186-acf7-4ef6-ba4f-9ac1910da458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.reset_index(drop = True, inplace = True) \n",
    "df_val.reset_index(drop = True, inplace = True)\n",
    "df_test.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a430c0e4-ba28-418d-8b64-7bfd04d736d9",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4ade2b19-3622-417a-80f1-eb7c01d20526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5680/5680 [00:46<00:00, 122.75it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:03<00:00, 62.28it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1640/1640 [00:04<00:00, 387.46it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5680/5680 [01:27<00:00, 65.12it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5680/5680 [00:04<00:00, 1399.84it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:02<00:00, 83.02it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:00<00:00, 1382.38it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1640/1640 [00:21<00:00, 76.98it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1640/1640 [00:00<00:00, 1871.66it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = feature_extractor(df_train, df_val, df_test, 13) # 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aa9f2177-0f8c-4acc-99d2-2229fadcd746",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_val, y_test = encode_labels(y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c1673cce-47be-4b0c-bae6-6495df831259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.size(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6f2cb031-c2ce-4918-97c9-e7932bb195b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = standard_scaling(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "93b8dc2d-994f-4dbb-bf24-3ba9f8b64105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5680, 157, 13)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77410203-cdfc-4e17-84c0-48f9c9fb8406",
   "metadata": {},
   "source": [
    "## Shuffle training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "54a83e4f-6491-48a4-a99f-9323ced83b31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfd4e5a-3975-4e42-bb85-fbcc0ee518e6",
   "metadata": {},
   "source": [
    "## Train with best parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9b7dbb1e-b47c-4ad7-9eb8-1ee353449b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Accuracy 0.9042315085728964 using {'lr': 0.0001, 'init_mode': 'uniform', 'batch_size': 4}\n",
    "def create_model( init_mode='uniform', lr = 0.0001):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(layers.Conv1D(256, 5,padding='same',\n",
    "                     input_shape=(157,13), kernel_initializer=init_mode)) # 157, 12\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=(4)))\n",
    "    model.add(layers.Dropout(0.6)) #0.6\n",
    "\n",
    "    model.add(layers.Conv1D(128, 5,padding='same', kernel_initializer=init_mode))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=(4)))\n",
    "    model.add(layers.Dropout(0.6)) #0.6\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, kernel_initializer=init_mode))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.add(layers.Activation('sigmoid'))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=Adam(lr = lr) , \n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e803ef82-a88f-4bff-8755-b5908c40ef4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fba4c955-4995-4a6f-bb6f-14011837b8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-28 22:50:18.438995: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-28 22:50:18.439516: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a27dc630-25bd-4e88-8cef-3c5180600082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b388358d-91ec-452b-b87f-e7d3fff98e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f46f3990-6b85-4f9d-8e77-fb75cfcb1a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e44efbae-1905-4215-b1d2-31c533975120",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-28 22:50:19.485380: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2021-09-28 22:50:19.485415: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2021-09-28 22:50:19.541446: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n"
     ]
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6303dfdb-8d2b-4253-867a-c1822feb8f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                                 factor=0.5, patience=4, \n",
    "                                                 verbose=1, mode='max', \n",
    "                                                 min_lr=0.000001)\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=45, \n",
    "                                              verbose=1, restore_best_weights = True )\n",
    "\n",
    "# classweight \n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "class_weights = {l:c for l,c in zip(np.unique(y_train), class_weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0225650f-cc9a-4d01-a48d-8be2a95ac048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-28 22:50:20.308732: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-28 22:50:20.330299: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2299965000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "  48/1420 [>.............................] - ETA: 5s - loss: 0.7269 - accuracy: 0.5485"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-28 22:50:20.863090: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2021-09-28 22:50:20.863115: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2021-09-28 22:50:20.869437: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.\n",
      "2021-09-28 22:50:20.872182: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2021-09-28 22:50:20.876011: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: logs/20210928-225019/train/plugins/profile/2021_09_28_22_50_20\n",
      "2021-09-28 22:50:20.876741: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to logs/20210928-225019/train/plugins/profile/2021_09_28_22_50_20/helemanc-Latitude-5410.trace.json.gz\n",
      "2021-09-28 22:50:20.884305: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: logs/20210928-225019/train/plugins/profile/2021_09_28_22_50_20\n",
      "2021-09-28 22:50:20.884516: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to logs/20210928-225019/train/plugins/profile/2021_09_28_22_50_20/helemanc-Latitude-5410.memory_profile.json.gz\n",
      "2021-09-28 22:50:20.884938: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/20210928-225019/train/plugins/profile/2021_09_28_22_50_20Dumped tool data for xplane.pb to logs/20210928-225019/train/plugins/profile/2021_09_28_22_50_20/helemanc-Latitude-5410.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/20210928-225019/train/plugins/profile/2021_09_28_22_50_20/helemanc-Latitude-5410.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/20210928-225019/train/plugins/profile/2021_09_28_22_50_20/helemanc-Latitude-5410.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/20210928-225019/train/plugins/profile/2021_09_28_22_50_20/helemanc-Latitude-5410.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/20210928-225019/train/plugins/profile/2021_09_28_22_50_20/helemanc-Latitude-5410.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1420/1420 [==============================] - 6s 4ms/step - loss: 0.6727 - accuracy: 0.5879 - val_loss: 0.7268 - val_accuracy: 0.5125\n",
      "Epoch 2/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.5564 - accuracy: 0.6784 - val_loss: 0.7187 - val_accuracy: 0.5708\n",
      "Epoch 3/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.5189 - accuracy: 0.7073 - val_loss: 0.7333 - val_accuracy: 0.5250\n",
      "Epoch 4/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.4818 - accuracy: 0.7355 - val_loss: 0.7565 - val_accuracy: 0.5500\n",
      "Epoch 5/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.4670 - accuracy: 0.7447 - val_loss: 0.7234 - val_accuracy: 0.5708\n",
      "Epoch 6/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.4438 - accuracy: 0.7586 - val_loss: 0.7093 - val_accuracy: 0.5833\n",
      "Epoch 7/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.4346 - accuracy: 0.7580 - val_loss: 0.7558 - val_accuracy: 0.5250\n",
      "Epoch 8/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.4355 - accuracy: 0.7454 - val_loss: 0.7127 - val_accuracy: 0.5625\n",
      "Epoch 9/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.4209 - accuracy: 0.7623 - val_loss: 0.6988 - val_accuracy: 0.5542\n",
      "Epoch 10/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.4043 - accuracy: 0.7768 - val_loss: 0.6932 - val_accuracy: 0.5792\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 11/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3996 - accuracy: 0.7657 - val_loss: 0.6979 - val_accuracy: 0.5542\n",
      "Epoch 12/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3858 - accuracy: 0.7793 - val_loss: 0.6981 - val_accuracy: 0.5792\n",
      "Epoch 13/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3782 - accuracy: 0.7803 - val_loss: 0.6921 - val_accuracy: 0.5792\n",
      "Epoch 14/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3801 - accuracy: 0.7839 - val_loss: 0.7072 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 15/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3627 - accuracy: 0.7972 - val_loss: 0.6993 - val_accuracy: 0.5583\n",
      "Epoch 16/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3796 - accuracy: 0.7821 - val_loss: 0.6953 - val_accuracy: 0.5792\n",
      "Epoch 17/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3719 - accuracy: 0.7852 - val_loss: 0.6836 - val_accuracy: 0.5667\n",
      "Epoch 18/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3802 - accuracy: 0.7763 - val_loss: 0.6838 - val_accuracy: 0.5833\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 19/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3605 - accuracy: 0.7910 - val_loss: 0.6849 - val_accuracy: 0.5708\n",
      "Epoch 20/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3644 - accuracy: 0.7955 - val_loss: 0.6845 - val_accuracy: 0.5792\n",
      "Epoch 21/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3682 - accuracy: 0.7814 - val_loss: 0.6844 - val_accuracy: 0.5708\n",
      "Epoch 22/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3598 - accuracy: 0.7943 - val_loss: 0.6788 - val_accuracy: 0.5792\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 23/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3557 - accuracy: 0.7983 - val_loss: 0.6843 - val_accuracy: 0.5667\n",
      "Epoch 24/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3660 - accuracy: 0.7897 - val_loss: 0.6855 - val_accuracy: 0.5875\n",
      "Epoch 25/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3601 - accuracy: 0.7931 - val_loss: 0.6816 - val_accuracy: 0.5750\n",
      "Epoch 26/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3509 - accuracy: 0.7954 - val_loss: 0.6789 - val_accuracy: 0.5750\n",
      "Epoch 27/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3620 - accuracy: 0.7957 - val_loss: 0.6788 - val_accuracy: 0.5792\n",
      "Epoch 28/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3566 - accuracy: 0.7945 - val_loss: 0.6791 - val_accuracy: 0.5625\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 29/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3572 - accuracy: 0.7897 - val_loss: 0.6797 - val_accuracy: 0.5708\n",
      "Epoch 30/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3469 - accuracy: 0.7894 - val_loss: 0.6796 - val_accuracy: 0.5667\n",
      "Epoch 31/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3577 - accuracy: 0.7943 - val_loss: 0.6807 - val_accuracy: 0.5708\n",
      "Epoch 32/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3631 - accuracy: 0.7927 - val_loss: 0.6809 - val_accuracy: 0.5583\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "Epoch 33/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3539 - accuracy: 0.8049 - val_loss: 0.6814 - val_accuracy: 0.5625\n",
      "Epoch 34/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3449 - accuracy: 0.7990 - val_loss: 0.6810 - val_accuracy: 0.5583\n",
      "Epoch 35/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3567 - accuracy: 0.7968 - val_loss: 0.6815 - val_accuracy: 0.5667\n",
      "Epoch 36/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3572 - accuracy: 0.7887 - val_loss: 0.6807 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "Epoch 37/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3644 - accuracy: 0.7791 - val_loss: 0.6805 - val_accuracy: 0.5708\n",
      "Epoch 38/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3567 - accuracy: 0.7954 - val_loss: 0.6807 - val_accuracy: 0.5667\n",
      "Epoch 39/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3463 - accuracy: 0.8000 - val_loss: 0.6804 - val_accuracy: 0.5667\n",
      "Epoch 40/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3457 - accuracy: 0.8001 - val_loss: 0.6807 - val_accuracy: 0.5667\n",
      "Epoch 41/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3579 - accuracy: 0.7905 - val_loss: 0.6812 - val_accuracy: 0.5667\n",
      "Epoch 42/500\n",
      "1420/1420 [==============================] - 6s 4ms/step - loss: 0.3603 - accuracy: 0.7893 - val_loss: 0.6809 - val_accuracy: 0.5667\n",
      "Epoch 43/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3563 - accuracy: 0.7950 - val_loss: 0.6814 - val_accuracy: 0.5667\n",
      "Epoch 44/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3499 - accuracy: 0.7963 - val_loss: 0.6818 - val_accuracy: 0.5667\n",
      "Epoch 45/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3526 - accuracy: 0.7914 - val_loss: 0.6821 - val_accuracy: 0.5667\n",
      "Epoch 46/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3582 - accuracy: 0.7866 - val_loss: 0.6820 - val_accuracy: 0.5667\n",
      "Epoch 47/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3624 - accuracy: 0.7845 - val_loss: 0.6826 - val_accuracy: 0.5667\n",
      "Epoch 48/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3546 - accuracy: 0.7917 - val_loss: 0.6830 - val_accuracy: 0.5667\n",
      "Epoch 49/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3500 - accuracy: 0.7981 - val_loss: 0.6829 - val_accuracy: 0.5625\n",
      "Epoch 50/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3487 - accuracy: 0.7994 - val_loss: 0.6826 - val_accuracy: 0.5625\n",
      "Epoch 51/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3517 - accuracy: 0.7923 - val_loss: 0.6827 - val_accuracy: 0.5625\n",
      "Epoch 52/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3454 - accuracy: 0.7981 - val_loss: 0.6828 - val_accuracy: 0.5625\n",
      "Epoch 53/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3651 - accuracy: 0.7872 - val_loss: 0.6826 - val_accuracy: 0.5625\n",
      "Epoch 54/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3513 - accuracy: 0.8038 - val_loss: 0.6825 - val_accuracy: 0.5625\n",
      "Epoch 55/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3491 - accuracy: 0.7972 - val_loss: 0.6825 - val_accuracy: 0.5583\n",
      "Epoch 56/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3689 - accuracy: 0.7828 - val_loss: 0.6831 - val_accuracy: 0.5542\n",
      "Epoch 57/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3473 - accuracy: 0.8087 - val_loss: 0.6828 - val_accuracy: 0.5625\n",
      "Epoch 58/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3476 - accuracy: 0.7909 - val_loss: 0.6829 - val_accuracy: 0.5583\n",
      "Epoch 59/500\n",
      "1420/1420 [==============================] - 6s 4ms/step - loss: 0.3608 - accuracy: 0.7834 - val_loss: 0.6831 - val_accuracy: 0.5625\n",
      "Epoch 60/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3507 - accuracy: 0.7969 - val_loss: 0.6833 - val_accuracy: 0.5625\n",
      "Epoch 61/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3550 - accuracy: 0.7958 - val_loss: 0.6828 - val_accuracy: 0.5625\n",
      "Epoch 62/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3562 - accuracy: 0.7876 - val_loss: 0.6827 - val_accuracy: 0.5625\n",
      "Epoch 63/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3657 - accuracy: 0.7903 - val_loss: 0.6824 - val_accuracy: 0.5625\n",
      "Epoch 64/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3532 - accuracy: 0.7970 - val_loss: 0.6831 - val_accuracy: 0.5625\n",
      "Epoch 65/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3559 - accuracy: 0.7958 - val_loss: 0.6829 - val_accuracy: 0.5583\n",
      "Epoch 66/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3492 - accuracy: 0.8003 - val_loss: 0.6826 - val_accuracy: 0.5583\n",
      "Epoch 67/500\n",
      "1420/1420 [==============================] - 5s 4ms/step - loss: 0.3520 - accuracy: 0.7975 - val_loss: 0.6828 - val_accuracy: 0.5542\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00067: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=4, epochs=500, validation_data=(X_val, y_val),\n",
    "           callbacks=[reduce_lr, early_stop, tensorboard_callback], class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "27f669bf-fff8-46a0-996d-ade8e7c92a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d261a7ab3aa2e4f9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d261a7ab3aa2e4f9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b8c62f64-3d7f-428d-a4b8-b28470eb55db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410/410 [==============================] - 1s 1ms/step - loss: 0.7384 - accuracy: 0.4762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7383779287338257, 0.47621950507164]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a88d9c9b-6ce3-4606-88ee-666e549a1a7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.99      0.62       716\n",
      "           1       0.89      0.08      0.15       924\n",
      "\n",
      "    accuracy                           0.48      1640\n",
      "   macro avg       0.67      0.53      0.38      1640\n",
      "weighted avg       0.70      0.48      0.35      1640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predictions = model.predict(X_test)\n",
    "pred = [1 * (x[0]>=0.50) for x in predictions] #0.5 o 0.52? \n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a527dbd4-2998-4d63-86e3-b92746ce7549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
