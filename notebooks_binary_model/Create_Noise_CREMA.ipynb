{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbfbda09-3f77-4a1f-a0b5-45a6093fb84c",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "NOTES: The warnings after the import are referred to the fact that Tensorflow 2.x versions are built to directly look for a GPU in the system. The warning can be forgot if you are not going to use the GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c655d152-dd51-437d-82c7-cdbe53b79366",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source myenv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4d20c73-5b1c-45ab-a0c7-758f133fc992",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-10 11:11:04.093670: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-10 11:11:04.093691: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-09-10 11:11:04.910051: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-10 11:11:04.910609: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-09-10 11:11:04.983024: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-09-10 11:11:04.983049: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (helemanc-Latitude-5410): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import IPython.display as ipd\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pickle\n",
    "import scipy\n",
    "import ipywidgets\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, AveragePooling1D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import SGD, Adam \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# from livelossplot import PlotLossesKeras\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b2c64b-b4d2-48b8-aa4d-5b12f6ea1d8d",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85158053-9d9d-472d-aea3-8f2b4a6cf70e",
   "metadata": {},
   "source": [
    "# Compute dataframes for datasets and split in Train, Val, Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13cb58e4-5602-4521-b11c-21a4090a9d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = '/media/helemanc/OS/Users/i2CAT/Desktop/Datasets SER/'\n",
    "TESS = os.path.join(main_path, \"tess/TESS Toronto emotional speech set data/\") \n",
    "RAV = os.path.join(main_path, \"ravdess-emotional-speech-audio/audio_speech_actors_01-24\")\n",
    "SAVEE = os.path.join(main_path, \"savee/ALL/\")\n",
    "CREMA = os.path.join(main_path, \"creamd/AudioWAV/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a77fdf27-e2eb-4628-a892-dcd8c7cb5805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "males = [1,\n",
    "5,\n",
    "11,\n",
    "14,\n",
    "15,\n",
    "16,\n",
    "17,\n",
    "19,\n",
    "22,\n",
    "23,\n",
    "26,\n",
    "27,\n",
    "31,\n",
    "32,\n",
    "33,\n",
    "34,\n",
    "35,\n",
    "36,\n",
    "38,\n",
    "39,\n",
    "41,\n",
    "42,\n",
    "44,\n",
    "45,\n",
    "48,\n",
    "50,\n",
    "51,\n",
    "57,\n",
    "59, \n",
    "62, \n",
    "64,\n",
    "65, \n",
    "66,\n",
    "67,\n",
    "68,\n",
    "69,\n",
    "70,\n",
    "71,\n",
    "77, \n",
    "80, \n",
    "81, \n",
    "83, \n",
    "85, \n",
    "86, \n",
    "87,\n",
    "88, \n",
    "90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "052f14b0-9f56-4bdb-af1c-66f22dc52f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "females = [ 2,\n",
    "3,\n",
    "4,\n",
    "6,\n",
    "7,\n",
    "8,\n",
    "9,\n",
    "10,\n",
    "12,\n",
    "13,\n",
    "18,\n",
    "20,\n",
    "21,\n",
    "24,\n",
    "25,\n",
    "28,\n",
    "29,\n",
    "30,\n",
    "37,\n",
    "40,\n",
    "43,\n",
    "46,\n",
    "47,\n",
    "49,\n",
    "52,\n",
    "53,\n",
    "54,\n",
    "55,\n",
    "56, \n",
    "58, \n",
    "60,\n",
    "61,\n",
    "63,\n",
    "72, \n",
    "73, \n",
    "74, \n",
    "75, \n",
    "76, \n",
    "78, \n",
    "79, \n",
    "82, \n",
    "84, \n",
    "89, \n",
    "91]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3b87dae-cafd-4cd3-8d1e-ed35f5f6873e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>actors</th>\n",
       "      <th>gender</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>happy</td>\n",
       "      <td>91</td>\n",
       "      <td>female</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>91</td>\n",
       "      <td>female</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>angry</td>\n",
       "      <td>91</td>\n",
       "      <td>female</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disgust</td>\n",
       "      <td>91</td>\n",
       "      <td>female</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fear</td>\n",
       "      <td>91</td>\n",
       "      <td>female</td>\n",
       "      <td>/media/helemanc/OS/Users/i2CAT/Desktop/Dataset...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emotion_label actors  gender  \\\n",
       "0         happy     91  female   \n",
       "1       sadness     91  female   \n",
       "2         angry     91  female   \n",
       "3       disgust     91  female   \n",
       "4          fear     91  female   \n",
       "\n",
       "                                                path  \n",
       "0  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "1  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "2  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "3  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  \n",
       "4  /media/helemanc/OS/Users/i2CAT/Desktop/Dataset...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crema_directory_list = os.listdir(CREMA)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "actors = []\n",
    "gender = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for file in crema_directory_list:\n",
    "\n",
    "    # storing file emotions\n",
    "    part=file.split('_')\n",
    "    \n",
    "    # use only high intensity files\n",
    "    if \"HI\" in part[3] :\n",
    "        actor = part[0][2:]\n",
    "        actors.append(actor)\n",
    "        if int(actor) in males:\n",
    "            gender.append('male')\n",
    "        else: \n",
    "            gender.append('female')\n",
    "    \n",
    "        # storing file paths\n",
    "        file_path.append(CREMA + file)\n",
    "        if part[2] == 'SAD':\n",
    "            file_emotion.append('sadness')\n",
    "        elif part[2] == 'ANG':\n",
    "            file_emotion.append('angry')\n",
    "        elif part[2] == 'DIS':\n",
    "            file_emotion.append('disgust')\n",
    "        elif part[2] == 'FEA':\n",
    "            file_emotion.append('fear')\n",
    "        elif part[2] == 'HAP':\n",
    "            file_emotion.append('happy')\n",
    "        elif part[2] == 'NEU':\n",
    "            file_emotion.append('neutral')\n",
    "        else:\n",
    "            file_emotion.append('Unknown')\n",
    "\n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['emotion_label'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['path'])\n",
    "actors_df = pd.DataFrame(actors, columns=['actors'])\n",
    "gender_df = pd.DataFrame(gender, columns=['gender'])                      \n",
    "Crema_df = pd.concat([emotion_df, actors_df, gender_df, path_df], axis=1)\n",
    "Crema_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baab239a-7ebc-4685-bcb7-c11e3c5afce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Crema_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "084e99b2-b993-4141-a41a-5f31660f1c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_files = {}\n",
    "\n",
    "for index, row in Crema_df.iterrows():\n",
    "    actor = row['actors']\n",
    "    if actor not in actor_files.keys(): \n",
    "        actor_files[actor] = 1\n",
    "    else: \n",
    "        actor_files[actor]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9ea1edb-575e-466e-aabd-913b2c0ee607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'91': 5,\n",
       " '90': 5,\n",
       " '89': 5,\n",
       " '88': 5,\n",
       " '87': 5,\n",
       " '86': 5,\n",
       " '85': 5,\n",
       " '84': 5,\n",
       " '83': 5,\n",
       " '82': 5,\n",
       " '81': 5,\n",
       " '80': 5,\n",
       " '79': 5,\n",
       " '78': 5,\n",
       " '77': 5,\n",
       " '76': 5,\n",
       " '75': 5,\n",
       " '74': 5,\n",
       " '73': 5,\n",
       " '72': 5,\n",
       " '71': 5,\n",
       " '70': 5,\n",
       " '69': 5,\n",
       " '68': 5,\n",
       " '67': 5,\n",
       " '66': 5,\n",
       " '65': 5,\n",
       " '64': 5,\n",
       " '63': 5,\n",
       " '62': 5,\n",
       " '61': 5,\n",
       " '60': 5,\n",
       " '59': 5,\n",
       " '58': 5,\n",
       " '57': 5,\n",
       " '56': 5,\n",
       " '55': 5,\n",
       " '54': 5,\n",
       " '53': 5,\n",
       " '52': 5,\n",
       " '51': 5,\n",
       " '50': 5,\n",
       " '49': 5,\n",
       " '48': 5,\n",
       " '47': 5,\n",
       " '46': 5,\n",
       " '45': 5,\n",
       " '44': 5,\n",
       " '43': 5,\n",
       " '42': 5,\n",
       " '41': 5,\n",
       " '40': 5,\n",
       " '39': 5,\n",
       " '38': 5,\n",
       " '37': 5,\n",
       " '36': 5,\n",
       " '35': 5,\n",
       " '34': 5,\n",
       " '33': 5,\n",
       " '32': 5,\n",
       " '31': 5,\n",
       " '30': 5,\n",
       " '29': 5,\n",
       " '28': 5,\n",
       " '27': 5,\n",
       " '26': 5,\n",
       " '25': 5,\n",
       " '24': 5,\n",
       " '23': 5,\n",
       " '22': 5,\n",
       " '21': 5,\n",
       " '20': 5,\n",
       " '19': 5,\n",
       " '18': 5,\n",
       " '17': 5,\n",
       " '16': 5,\n",
       " '15': 5,\n",
       " '14': 5,\n",
       " '13': 5,\n",
       " '12': 5,\n",
       " '11': 5,\n",
       " '10': 5,\n",
       " '09': 5,\n",
       " '08': 5,\n",
       " '07': 5,\n",
       " '06': 5,\n",
       " '05': 5,\n",
       " '04': 5,\n",
       " '03': 5,\n",
       " '02': 5,\n",
       " '01': 5}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eddebb0-e7c6-45e4-a80a-584fc31992df",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_males = 0 \n",
    "count_females = 0 \n",
    "male_list = []\n",
    "for index, row in Crema_df.iterrows(): \n",
    "    gender = row['gender']\n",
    "    actor = row['actors']\n",
    "    if gender == 'male':\n",
    "        count_males +=1\n",
    "        if actor not in male_list: \n",
    "            male_list.append(actor)\n",
    "    else: \n",
    "        count_females +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96ea9f51-1482-46c5-b11f-d6491db39f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 220)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_males, count_females"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e1353-483c-4cc6-948e-f75eab32c898",
   "metadata": {},
   "source": [
    "Since there are more males than females we will remove randomly 3 male actors (since there are exactly 5 audio files per actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef58399c-b555-4227-865a-20b82bf67b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['17', '80', '88']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "random.seed(42)\n",
    "males_to_remove = random.sample(male_list, 3)\n",
    "males_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e3a2687-f5f7-42d2-b965-d7247389887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = []\n",
    "for index, row in Crema_df.iterrows(): \n",
    "    if row['actors'] not in males_to_remove: \n",
    "        new_df.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e17b9aa0-4a62-476f-9b61-a62f1dc29574",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREMA_df = pd.DataFrame(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85723691-5092-4274-aec7-b31d3dc97c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in CREMA_df.iterrows(): \n",
    "    if row['actors'] == '17': \n",
    "        print(\"Elements not removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1b46201-cec8-45fd-9b40-2e7bbac29279",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_males = 0 \n",
    "count_females = 0 \n",
    "male_list = []\n",
    "female_list = []\n",
    "for index, row in CREMA_df.iterrows(): \n",
    "    gender = row['gender']\n",
    "    actor = row['actors']\n",
    "    if gender == 'male':\n",
    "        count_males +=1\n",
    "        if actor not in male_list: \n",
    "            male_list.append(actor)\n",
    "    else: \n",
    "        count_females +=1\n",
    "        if actor not in female_list: \n",
    "            female_list.append(actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd50b603-875b-4299-9671-1ccdf47c2733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 220)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_males, count_females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a533e46-b9cc-44ee-a9c0-02349ffc693c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(female_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3c3291d-e142-4b01-8a25-b9344ab9a80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(male_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08fa89d1-2b5d-437e-b043-6471f38ac919",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREMA_train = []\n",
    "CREMA_val = []\n",
    "CREMA_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "032cb2a3-67e1-41a6-a6be-a5a0561ee4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "females_train = random.sample(female_list, 32)\n",
    "males_train = random.sample(male_list, 32)\n",
    "\n",
    "# remove the elements assigned to train \n",
    "for element in females_train:\n",
    "    if element in female_list:\n",
    "        female_list.remove(element)\n",
    "        \n",
    "for element in males_train:\n",
    "    if element in male_list:\n",
    "        male_list.remove(element)\n",
    "\n",
    "         \n",
    "females_val = random.sample(female_list, 6) \n",
    "males_val = random.sample(male_list, 6) \n",
    "\n",
    "# remove the elements assigned to val\n",
    "for element in females_val:\n",
    "    if element in female_list:\n",
    "        female_list.remove(element)\n",
    "        \n",
    "for element in males_val:\n",
    "    if element in male_list:\n",
    "        male_list.remove(element)\n",
    "        \n",
    "females_test = random.sample(female_list, 6) \n",
    "males_test = random.sample(male_list, 6)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c7d4d95-8a66-470d-af10-974dd5217fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['54',\n",
       "  '56',\n",
       "  '58',\n",
       "  '74',\n",
       "  '76',\n",
       "  '13',\n",
       "  '78',\n",
       "  '29',\n",
       "  '84',\n",
       "  '89',\n",
       "  '09',\n",
       "  '60',\n",
       "  '04',\n",
       "  '55',\n",
       "  '52',\n",
       "  '91',\n",
       "  '02',\n",
       "  '07',\n",
       "  '46',\n",
       "  '49',\n",
       "  '37',\n",
       "  '10',\n",
       "  '20',\n",
       "  '75',\n",
       "  '21',\n",
       "  '53',\n",
       "  '06',\n",
       "  '28',\n",
       "  '18',\n",
       "  '63',\n",
       "  '30',\n",
       "  '03'],\n",
       " ['57',\n",
       "  '69',\n",
       "  '65',\n",
       "  '45',\n",
       "  '77',\n",
       "  '81',\n",
       "  '41',\n",
       "  '15',\n",
       "  '44',\n",
       "  '23',\n",
       "  '59',\n",
       "  '86',\n",
       "  '34',\n",
       "  '01',\n",
       "  '85',\n",
       "  '66',\n",
       "  '31',\n",
       "  '33',\n",
       "  '05',\n",
       "  '48',\n",
       "  '50',\n",
       "  '67',\n",
       "  '51',\n",
       "  '22',\n",
       "  '36',\n",
       "  '87',\n",
       "  '71',\n",
       "  '39',\n",
       "  '42',\n",
       "  '11',\n",
       "  '32',\n",
       "  '14'],\n",
       " ['43', '61', '40', '47', '73', '24'],\n",
       " ['62', '68', '64', '83', '70', '26'],\n",
       " ['08', '79', '12', '25', '72', '82'],\n",
       " ['16', '19', '38', '35', '27', '90'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "females_train, males_train, females_val, males_val, females_test, males_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2939ced3-11b2-4b4b-bf8a-44109750c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = females_train + males_train \n",
    "val = females_val + males_val \n",
    "test = females_test + males_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bb0d67b-6806-42d3-8731-9bec6a7a09da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index, row in CREMA_df.iterrows(): \n",
    "    gender = row['gender']\n",
    "    actor = row['actors']\n",
    "    if actor in train: \n",
    "        CREMA_train.append(row)\n",
    "    elif actor in val: \n",
    "        CREMA_val.append(row)\n",
    "    else:\n",
    "        CREMA_test.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "952ca543-e161-44b2-a4ea-7c2e128ff352",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREMA_train = pd.DataFrame(CREMA_train) \n",
    "CREMA_val = pd.DataFrame(CREMA_val) \n",
    "CREMA_test = pd.DataFrame(CREMA_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9920720b-3692-4c74-9d02-61d14034c08d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((320, 4), (60, 4), (60, 4))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CREMA_train.shape, CREMA_val.shape, CREMA_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53b78026-8981-4e94-beb8-ad71616f3a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CREMA_train = CREMA_train.reset_index(drop=True) \n",
    "CREMA_val = CREMA_val.reset_index(drop = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65fd1e9f-4557-49c6-87de-741ec23d0be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = CREMA_train \n",
    "df_val = CREMA_val \n",
    "df_test = CREMA_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a6426f-e443-479c-98cf-bd8ca3bd4403",
   "metadata": {},
   "source": [
    "# Create Noise Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f77795b6-7cba-422f-ad99-0d313c8d6a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import random \n",
    "from pydub.utils import make_chunks\n",
    "\n",
    "\n",
    "def create_noise_files(df_train, df_val, df_test): \n",
    "    \n",
    "    '''\n",
    "    Apply noise only on training files, so double the number of training files and keep \n",
    "    validation and test the same\n",
    "    '''\n",
    "    path_noise_sound_1 = '/home/helemanc/Desktop/Binary_Model/noise_sounds/freight_train.wav'\n",
    "    path_noise_sound_2 = '/home/helemanc/Desktop/Binary_Model/noise_sounds/inside_train.wav'\n",
    "    path_noise_sound_3 = '/home/helemanc/Desktop/Binary_Model/noise_sounds/small_crowd.wav'\n",
    "    \n",
    "    path_noise_dataset_train = '/home/helemanc/Desktop/Binary_Model/noise_datasets/crema/train'\n",
    "    #path_noise_dataset_val = '/home/helemanc/Desktop/Binary_Model/noise_datasets/ravdess/val'\n",
    "    #path_noise_dataset_test = '/home/helemanc/Desktop/Binary_Model/noise_datasets/ravdess/test'\n",
    "    \n",
    "\n",
    "    #df_list = [df_train, df_val, df_test]\n",
    "    #count_df = 0 \n",
    "    \n",
    "    train_emotions = []\n",
    "    train_genders = []\n",
    "    train_paths = []\n",
    "    \n",
    "    #val_emotions = []\n",
    "    #val_genders = []\n",
    "    #val_paths = []\n",
    "    \n",
    "    #test_emotions = []\n",
    "    #test_genders = []\n",
    "    #test_paths = []\n",
    "    \n",
    "    #for df in df_list: \n",
    "        \n",
    "    for index, row in tqdm(df_train.iterrows()): \n",
    "        path = row['path']\n",
    "        sound1 = AudioSegment.from_file(path)\n",
    "        samples, sr = librosa.load(path, res_type='kaiser_fast', sr=16000)\n",
    "        duration = librosa.get_duration(y = samples, sr = sr)\n",
    "\n",
    "        # pick a noise sound file randomly \n",
    "        noise_list = [path_noise_sound_1, path_noise_sound_2, path_noise_sound_3]\n",
    "        random_noise = random.choice(noise_list) \n",
    "\n",
    "        lower_volume = 0 \n",
    "\n",
    "        # adjust volume to not cover the voice of the audio file \n",
    "        # warning: different levels of dB need to be calibrate for each dataset \n",
    "        '''\n",
    "        if random_noise == path_noise_sound_1: \n",
    "            lower_volume = 40\n",
    "        elif random_noise == path_noise_sound_2: \n",
    "            lower_volume = 25 \n",
    "        else: \n",
    "            lower_volume = 40\n",
    "        '''\n",
    "\n",
    "        # other strategy: \n",
    "        # compute db of both files, compute the difference, and lower the volume of the file to make it \n",
    "        # a bit lower than the original file -almost equal- \n",
    "\n",
    "        sound2 = AudioSegment.from_file(random_noise)\n",
    "\n",
    "        # make chunks of duration equal to the audio file \n",
    "        chunk_length_ms = duration*1000 #ms\n",
    "        chunks = make_chunks(sound2, chunk_length_ms) \n",
    "\n",
    "        # pick a random chunk \n",
    "        random_chunk = random.choice(chunks)\n",
    "        difference = random_chunk.dBFS - sound1.dBFS\n",
    "\n",
    "        abs_difference = abs(difference)\n",
    "\n",
    "        lower = random_chunk - abs_difference - 2\n",
    "\n",
    "        # lower the volume of the noise file to be overlayed with the voice_sound \n",
    "        #lower = random_chunk - lower_volume\n",
    "\n",
    "        combined = sound1.overlay(lower)\n",
    "\n",
    "        parts = path.split('/')\n",
    "        fname = parts[-1]\n",
    "        \n",
    "        new_path = path_noise_dataset_train + '/' + fname \n",
    "\n",
    "        train_emotions.append(row['emotion_label'])\n",
    "        train_genders.append(row['gender'])\n",
    "        train_paths.append(new_path)\n",
    "\n",
    "        '''\n",
    "        if count_df == 0: \n",
    "            new_path = path_noise_dataset_train + '/' + fname \n",
    "\n",
    "            train_emotions.append(row['emotion_label'])\n",
    "            train_genders.append(row['gender'])\n",
    "            train_paths.append(new_path)\n",
    "\n",
    "        elif count_df == 1: \n",
    "            new_path = path_noise_dataset_val + '/' + fname\n",
    "\n",
    "            val_emotions.append(row['emotion_label'])\n",
    "            val_genders.append(row['gender'])\n",
    "            val_paths.append(new_path)\n",
    "\n",
    "        elif count_df == 2:\n",
    "            new_path = path_noise_dataset_test + '/' + fname          \n",
    "\n",
    "            test_emotions.append(row['emotion_label'])\n",
    "            test_genders.append(row['gender'])\n",
    "            test_paths.append(new_path)\n",
    "        '''\n",
    "        combined.export(new_path, format= 'wav')\n",
    "\n",
    "    #count_df +=1\n",
    "\n",
    "    df_train_noise = pd.DataFrame([train_emotions, train_genders, train_paths]).T\n",
    "    df_train_noise.columns = ['emotion_label', 'gender', 'path']\n",
    "    \n",
    "    #df_val_noise = pd.DataFrame([val_emotions, val_genders, val_paths]).T\n",
    "    #df_val_noise.columns = ['emotion_label', 'gender', 'path']\n",
    "    \n",
    "    #df_test_noise = pd.DataFrame([test_emotions, test_genders, test_paths]).T\n",
    "    #df_test_noise.columns = ['emotion_label', 'gender', 'path']\n",
    "\n",
    "    df_train_combined = pd.concat([df_train, df_train_noise])\n",
    "    df_train_combined.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #df_val_combined = pd.concat([df_val, df_val_noise])\n",
    "    #df_val_combined.reset_index(drop=True, inplace=True)\n",
    "                                   \n",
    "    #df_test_combined = pd.concat([df_test, df_test_noise])\n",
    "    #df_test_combined.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df_train_combined, df_val, df_test\n",
    "# have to save df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efb53f6b-9529-4c17-8d21-e731f3df9273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "320it [00:00, 351.24it/s]\n"
     ]
    }
   ],
   "source": [
    "new_df_train, new_df_val, new_df_test = create_noise_files(df_train, df_val, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e5a3a60-b70b-4837-85cb-0c1a7c86a599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((640, 4), (60, 4), (60, 4))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_train.shape, new_df_val.shape, new_df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e4af99-06eb-4f1f-b37a-e14446388f98",
   "metadata": {},
   "source": [
    "## Save dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11e5e91f-61e5-44f0-b419-a1f321fdb8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_path = \"/home/helemanc/Desktop/Binary_Model/df_csv_noise/crema\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29c28f6d-bd83-4033-bf86-133abd8e481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_train.to_csv(os.path.join(preprocess_path,\"df_train.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4efd7885-47ba-4703-ab44-8ef6553306c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df_val.to_csv(os.path.join(preprocess_path,\"df_val.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "995422ee-4907-4215-976c-0a9783ab69e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_test.to_csv(os.path.join(preprocess_path,\"df_test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e024a3d6-5e43-4853-afec-1b637e5aa045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
