{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5d6a5f1-2286-43cc-a59e-6bfaf1c19353",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configuration\n",
    "\n",
    "NOTES: The warnings after the import are referred to the fact that Tensorflow 2.x versions are built to directly look for a GPU in the system. The warning can be forgot if you are not going to use the GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1915ef1-d4d1-4cfa-94a9-46745a721df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source myenv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0e8b2f3-36b3-40fb-9524-103375a56c26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import IPython.display as ipd\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pickle\n",
    "import scipy\n",
    "import ipywidgets\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, AveragePooling1D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import SGD, Adam \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# from livelossplot import PlotLossesKeras\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50b2fa5-adb6-4468-b517-43719a47e1c5",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030b347b-fd96-4bb1-9213-fb6d5135f670",
   "metadata": {},
   "source": [
    "# Compute dataframes for datasets and split in Train, Val, Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7a62f7d-db5b-4364-899f-3d1e0b36727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = '/media/helemanc/OS/Users/i2CAT/Desktop/Datasets SER/'\n",
    "TESS = os.path.join(main_path, \"tess/TESS Toronto emotional speech set data/\") \n",
    "RAV = os.path.join(main_path, \"ravdess-emotional-speech-audio/audio_speech_actors_01-24\")\n",
    "SAVEE = os.path.join(main_path, \"savee/ALL/\")\n",
    "CREMA = os.path.join(main_path, \"creamd/AudioWAV/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c16c32cb-2337-42c2-a77f-f921364c133a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "angry       1200\n",
       "fear         800\n",
       "surprise     800\n",
       "sadness      800\n",
       "disgust      800\n",
       "neutral      800\n",
       "happy        400\n",
       "Name: emotion_label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_list = os.listdir(TESS)\n",
    "dir_list.sort()\n",
    "dir_list\n",
    "\n",
    "path = []\n",
    "emotion = []\n",
    "gender = []\n",
    "actors = []\n",
    "\n",
    "for i in dir_list:\n",
    "    fname = os.listdir(TESS + i)\n",
    "    for f in fname:\n",
    "        if i == 'OAF_angry':\n",
    "            emotion.append('angry')\n",
    "            gender.append('female')\n",
    "            actors.append('OAF')\n",
    "        elif i == 'YAF_angry': \n",
    "            emotion.append('angry')\n",
    "            gender.append('female')\n",
    "            actors.append('YAF')\n",
    "            \n",
    "            \n",
    "        elif i == 'OAF_disgust' :\n",
    "            emotion.append('disgust')\n",
    "            gender.append('female')\n",
    "            actors.append('OAF')\n",
    "        elif i == 'YAF_disgust': \n",
    "            emotion.append('disgust')\n",
    "            gender.append('female')\n",
    "            actors.append('YAF')\n",
    "            \n",
    "            \n",
    "        elif i == 'OAF_Fear':\n",
    "            emotion.append('fear')\n",
    "            gender.append('female')\n",
    "            actors.append('OAF')\n",
    "        elif i == 'YAF_fear': \n",
    "            emotion.append('fear')\n",
    "            gender.append('female')\n",
    "            actors.append('YAF') \n",
    "            \n",
    "            \n",
    "        elif i == 'OAF_happy' :\n",
    "            emotion.append('happy')\n",
    "            gender.append('female')\n",
    "            actors.append('OAF')\n",
    "        elif i == 'YAF_happy': \n",
    "            emotion.append('angry')\n",
    "            gender.append('female')\n",
    "            actors.append('YAF')            \n",
    "            \n",
    "        elif i == 'OAF_neutral':\n",
    "            emotion.append('neutral')\n",
    "            gender.append('female')\n",
    "            actors.append('OAF')   \n",
    "        elif i == 'YAF_neutral': \n",
    "            emotion.append('neutral')\n",
    "            gender.append('female')\n",
    "            actors.append('YAF')      \n",
    "            \n",
    "                \n",
    "        elif i == 'OAF_Pleasant_surprise':\n",
    "            emotion.append('surprise')\n",
    "            gender.append('female')\n",
    "            actors.append('OAF')\n",
    "        \n",
    "        elif i == 'YAF_pleasant_surprised': \n",
    "            emotion.append('surprise')\n",
    "            gender.append('female')\n",
    "            actors.append('YAF')            \n",
    "            \n",
    "        elif i == 'OAF_Sad':\n",
    "            emotion.append('sadness')\n",
    "            gender.append('female')\n",
    "            actors.append('OAF')\n",
    "        elif i == 'YAF_sad': \n",
    "            emotion.append('sadness')\n",
    "            gender.append('female')\n",
    "            actors.append('YAF')            \n",
    "        else:\n",
    "            emotion.append('Unknown')\n",
    "        path.append(TESS + i + \"/\" + f)\n",
    "\n",
    "TESS_df = pd.DataFrame(emotion, columns = ['emotion_label'])\n",
    "TESS_df = pd.concat([TESS_df, pd.DataFrame(gender, columns = ['gender']), \n",
    "                     pd.DataFrame(actors, columns= ['actors']),\n",
    "                     pd.DataFrame(path, columns = ['path'])],axis=1)\n",
    "TESS_df.emotion_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34328cb9-f26a-4448-ae04-58ba38c16380",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TESS_df= TESS_df[~TESS_df.path.str.contains('noise')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f5ff8e1-1ca0-4336-913d-493e56805ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESS_train = []\n",
    "TESS_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feeab675-0f91-473e-99c9-0a03e141d65c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 1400)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in TESS_df.iterrows(): \n",
    "    if row['actors'] == 'YAF': \n",
    "        TESS_train.append(row)\n",
    "    else: \n",
    "        TESS_test.append(row)\n",
    "len(TESS_train), len(TESS_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a89a772-8567-44cd-9e88-c87197717be7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(TESS_train)\n",
    "df_test = pd.DataFrame(TESS_test)\n",
    "df_val = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61c7bd7-1101-42e7-b066-669b07ab7dfd",
   "metadata": {},
   "source": [
    "# Create Noise Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48433196-d6b5-45af-92e6-1f557fa1a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import random \n",
    "from pydub.utils import make_chunks\n",
    "\n",
    "\n",
    "def create_noise_files(df_train, df_val, df_test): \n",
    "    \n",
    "    '''\n",
    "    Apply noise only on training files, so double the number of training files and keep \n",
    "    validation and test the same\n",
    "    '''\n",
    "    path_noise_sound_1 = '/home/helemanc/Desktop/Binary_Model/noise_sounds/freight_train.wav'\n",
    "    path_noise_sound_2 = '/home/helemanc/Desktop/Binary_Model/noise_sounds/inside_train.wav'\n",
    "    path_noise_sound_3 = '/home/helemanc/Desktop/Binary_Model/noise_sounds/small_crowd.wav'\n",
    "    \n",
    "    path_noise_dataset_train = '/home/helemanc/Desktop/Binary_Model/noise_datasets/tess/train'\n",
    "    #path_noise_dataset_val = '/home/helemanc/Desktop/Binary_Model/noise_datasets/ravdess/val'\n",
    "    #path_noise_dataset_test = '/home/helemanc/Desktop/Binary_Model/noise_datasets/ravdess/test'\n",
    "    \n",
    "\n",
    "    #df_list = [df_train, df_val, df_test]\n",
    "    #count_df = 0 \n",
    "    \n",
    "    train_emotions = []\n",
    "    train_genders = []\n",
    "    train_paths = []\n",
    "    \n",
    "    #val_emotions = []\n",
    "    #val_genders = []\n",
    "    #val_paths = []\n",
    "    \n",
    "    #test_emotions = []\n",
    "    #test_genders = []\n",
    "    #test_paths = []\n",
    "    \n",
    "    #for df in df_list: \n",
    "        \n",
    "    for index, row in tqdm(df_train.iterrows()): \n",
    "        path = row['path']\n",
    "        sound1 = AudioSegment.from_file(path)\n",
    "        samples, sr = librosa.load(path, res_type='kaiser_fast', sr=16000)\n",
    "        duration = librosa.get_duration(y = samples, sr = sr)\n",
    "\n",
    "        # pick a noise sound file randomly \n",
    "        noise_list = [path_noise_sound_1, path_noise_sound_2, path_noise_sound_3]\n",
    "        random_noise = random.choice(noise_list) \n",
    "\n",
    "        lower_volume = 0 \n",
    "\n",
    "        # adjust volume to not cover the voice of the audio file \n",
    "        # warning: different levels of dB need to be calibrate for each dataset \n",
    "        '''\n",
    "        if random_noise == path_noise_sound_1: \n",
    "            lower_volume = 40\n",
    "        elif random_noise == path_noise_sound_2: \n",
    "            lower_volume = 25 \n",
    "        else: \n",
    "            lower_volume = 40\n",
    "        '''\n",
    "\n",
    "        # other strategy: \n",
    "        # compute db of both files, compute the difference, and lower the volume of the file to make it \n",
    "        # a bit lower than the original file -almost equal- \n",
    "\n",
    "        sound2 = AudioSegment.from_file(random_noise)\n",
    "\n",
    "        # make chunks of duration equal to the audio file \n",
    "        chunk_length_ms = duration*1000 #ms\n",
    "        chunks = make_chunks(sound2, chunk_length_ms) \n",
    "\n",
    "        # pick a random chunk \n",
    "        random_chunk = random.choice(chunks)\n",
    "        difference = random_chunk.dBFS - sound1.dBFS\n",
    "\n",
    "        abs_difference = abs(difference)\n",
    "\n",
    "        lower = random_chunk - abs_difference - 2\n",
    "\n",
    "        # lower the volume of the noise file to be overlayed with the voice_sound \n",
    "        #lower = random_chunk - lower_volume\n",
    "\n",
    "        combined = sound1.overlay(lower)\n",
    "\n",
    "        parts = path.split('/')\n",
    "        fname = parts[-1]\n",
    "        \n",
    "        new_path = path_noise_dataset_train + '/' + fname \n",
    "\n",
    "        train_emotions.append(row['emotion_label'])\n",
    "        train_genders.append(row['gender'])\n",
    "        train_paths.append(new_path)\n",
    "\n",
    "        '''\n",
    "        if count_df == 0: \n",
    "            new_path = path_noise_dataset_train + '/' + fname \n",
    "\n",
    "            train_emotions.append(row['emotion_label'])\n",
    "            train_genders.append(row['gender'])\n",
    "            train_paths.append(new_path)\n",
    "\n",
    "        elif count_df == 1: \n",
    "            new_path = path_noise_dataset_val + '/' + fname\n",
    "\n",
    "            val_emotions.append(row['emotion_label'])\n",
    "            val_genders.append(row['gender'])\n",
    "            val_paths.append(new_path)\n",
    "\n",
    "        elif count_df == 2:\n",
    "            new_path = path_noise_dataset_test + '/' + fname          \n",
    "\n",
    "            test_emotions.append(row['emotion_label'])\n",
    "            test_genders.append(row['gender'])\n",
    "            test_paths.append(new_path)\n",
    "        '''\n",
    "        combined.export(new_path, format= 'wav')\n",
    "\n",
    "    #count_df +=1\n",
    "\n",
    "    df_train_noise = pd.DataFrame([train_emotions, train_genders, train_paths]).T\n",
    "    df_train_noise.columns = ['emotion_label', 'gender', 'path']\n",
    "    \n",
    "    #df_val_noise = pd.DataFrame([val_emotions, val_genders, val_paths]).T\n",
    "    #df_val_noise.columns = ['emotion_label', 'gender', 'path']\n",
    "    \n",
    "    #df_test_noise = pd.DataFrame([test_emotions, test_genders, test_paths]).T\n",
    "    #df_test_noise.columns = ['emotion_label', 'gender', 'path']\n",
    "\n",
    "    df_train_combined = pd.concat([df_train, df_train_noise])\n",
    "    df_train_combined.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #df_val_combined = pd.concat([df_val, df_val_noise])\n",
    "    #df_val_combined.reset_index(drop=True, inplace=True)\n",
    "                                   \n",
    "    #df_test_combined = pd.concat([df_test, df_test_noise])\n",
    "    #df_test_combined.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df_train_combined, df_val, df_test\n",
    "# have to save df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4de8adfc-3e4f-4d90-b96b-3b34f18c565f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1400it [00:20, 67.34it/s]\n"
     ]
    }
   ],
   "source": [
    "new_df_train, new_df_val, new_df_test = create_noise_files(df_train, df_val, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d88637d-6deb-4333-bc85-825ee6955066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2800, 4), (1400, 4))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_train.shape, new_df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfe0fb6-e309-457b-b12d-610e6a09f3a0",
   "metadata": {},
   "source": [
    "## Save dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fcc2c1c3-51de-467a-8e72-4867b0a11630",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_path = \"/home/helemanc/Desktop/Binary_Model/df_csv_noise/tess\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f0d8287-3cbe-45dc-ac1e-b6978a07fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_train.to_csv(os.path.join(preprocess_path,\"df_train.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a58207f-4c23-4f3f-a33e-400137ccd31c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df_test.to_csv(os.path.join(preprocess_path,\"df_test.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
