{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import pandas as pd \n",
    "import pandas as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "MODELS_FOLDER = \"models\"\n",
    "SCALERS_FOLDER = \"scalers\"\n",
    "\n",
    "# METTERE PATH MODELS E SCLAERS E CALCOLARE LA FINAL PREDICTION DIVERSAMENTE PRENDENDENDO PEZZI DI CODICE DELL'ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils\n",
    "import scipy\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.io.wavfile import write\n",
    "#from utils import resample, denoise\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "# print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Clean Data - Compute dataframes for datasets and split in Train, Val, Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = '/Users/helemanc/Documents/MasterAI/THESIS/Datasets SER'\n",
    "TESS = os.path.join(main_path, \"tess/TESS Toronto emotional speech set data/\") \n",
    "RAV = os.path.join(main_path, \"ravdess-emotional-speech-audio/audio_speech_actors_01-24\")\n",
    "SAVEE = os.path.join(main_path, \"savee/ALL/\")\n",
    "CREMA = os.path.join(main_path, \"creamd/AudioWAV/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RAVDESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:00, 191.56it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = []\n",
    "emotion = []\n",
    "voc_channel = []\n",
    "full_path = []\n",
    "modality = []\n",
    "intensity = []\n",
    "actors = []\n",
    "phrase =[]\n",
    "\n",
    "for root, dirs, files in tqdm(os.walk(RAV)):\n",
    "    for file in files:\n",
    "        try:\n",
    "            #Load librosa array, obtain mfcss, store the file and the mfcss information in a new array\n",
    "            # X, sample_rate = librosa.load(os.path.join(root,file), res_type='kaiser_fast')\n",
    "            # mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
    "            # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
    "            # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
    "           \n",
    "            modal = int(file[1:2])\n",
    "            vchan = int(file[4:5])\n",
    "            lab = int(file[7:8])\n",
    "            ints = int(file[10:11])\n",
    "            phr = int(file[13:14])\n",
    "            act = int(file[18:20])\n",
    "            # arr = mfccs, lab\n",
    "            # lst.append(arr)\n",
    "            \n",
    "            modality.append(modal)\n",
    "            voc_channel.append(vchan)\n",
    "            emotion.append(lab) #only labels\n",
    "            intensity.append(ints)\n",
    "            phrase.append(phr)\n",
    "            actors.append(act)\n",
    "            \n",
    "            full_path.append((root, file)) # only files\n",
    "          # If the file is not valid, skip it\n",
    "        except ValueError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised\n",
    "# merge neutral and calm\n",
    "emotions_list = ['neutral', 'neutral', 'happy', 'sadness', 'angry', 'fear', 'disgust', 'surprise']\n",
    "emotion_dict = {em[0]+1:em[1] for em in enumerate(emotions_list)}\n",
    "\n",
    "df = pd.DataFrame([emotion, voc_channel, modality, intensity, actors, actors,phrase, full_path]).T\n",
    "df.columns = ['emotion', 'voc_channel', 'modality', 'intensity', 'actors', 'gender', 'phrase', 'path']\n",
    "df['emotion'] = df['emotion'].map(emotion_dict)\n",
    "df['voc_channel'] = df['voc_channel'].map({1: 'speech', 2:'song'})\n",
    "df['modality'] = df['modality'].map({1: 'full AV', 2:'video only', 3:'audio only'})\n",
    "df['intensity'] = df['intensity'].map({1: 'normal', 2:'strong'})\n",
    "df['actors'] = df['actors']\n",
    "df['gender'] = df['actors'].apply(lambda x: 'female' if x%2 == 0 else 'male')\n",
    "df['phrase'] = df['phrase'].map({1: 'Kids are talking by the door', 2:'Dogs are sitting by the door'})\n",
    "df['path'] = df['path'].apply(lambda x: x[0] + '/' + x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove files with noise to apply the same noise to all files for data augmentation \n",
    "df = df[~df.path.str.contains('noise')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>voc_channel</th>\n",
       "      <th>modality</th>\n",
       "      <th>intensity</th>\n",
       "      <th>actors</th>\n",
       "      <th>gender</th>\n",
       "      <th>phrase</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>speech</td>\n",
       "      <td>audio only</td>\n",
       "      <td>normal</td>\n",
       "      <td>16</td>\n",
       "      <td>female</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fear</td>\n",
       "      <td>speech</td>\n",
       "      <td>audio only</td>\n",
       "      <td>normal</td>\n",
       "      <td>16</td>\n",
       "      <td>female</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fear</td>\n",
       "      <td>speech</td>\n",
       "      <td>audio only</td>\n",
       "      <td>strong</td>\n",
       "      <td>16</td>\n",
       "      <td>female</td>\n",
       "      <td>Kids are talking by the door</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angry</td>\n",
       "      <td>speech</td>\n",
       "      <td>audio only</td>\n",
       "      <td>strong</td>\n",
       "      <td>16</td>\n",
       "      <td>female</td>\n",
       "      <td>Kids are talking by the door</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>disgust</td>\n",
       "      <td>speech</td>\n",
       "      <td>audio only</td>\n",
       "      <td>normal</td>\n",
       "      <td>16</td>\n",
       "      <td>female</td>\n",
       "      <td>Kids are talking by the door</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion voc_channel    modality intensity actors  gender  \\\n",
       "0    angry      speech  audio only    normal     16  female   \n",
       "1     fear      speech  audio only    normal     16  female   \n",
       "2     fear      speech  audio only    strong     16  female   \n",
       "3    angry      speech  audio only    strong     16  female   \n",
       "5  disgust      speech  audio only    normal     16  female   \n",
       "\n",
       "                         phrase  \\\n",
       "0  Dogs are sitting by the door   \n",
       "1  Dogs are sitting by the door   \n",
       "2  Kids are talking by the door   \n",
       "3  Kids are talking by the door   \n",
       "5  Kids are talking by the door   \n",
       "\n",
       "                                                path  \n",
       "0  /Users/helemanc/Documents/MasterAI/THESIS/Data...  \n",
       "1  /Users/helemanc/Documents/MasterAI/THESIS/Data...  \n",
       "2  /Users/helemanc/Documents/MasterAI/THESIS/Data...  \n",
       "3  /Users/helemanc/Documents/MasterAI/THESIS/Data...  \n",
       "5  /Users/helemanc/Documents/MasterAI/THESIS/Data...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only speech\n",
    "RAV_df = df\n",
    "RAV_df = RAV_df.loc[RAV_df.voc_channel == 'speech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAV_df.insert(0, \"emotion_label\", RAV_df.emotion, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAV_df = RAV_df.drop(['emotion', 'voc_channel', 'modality', 'intensity', 'phrase'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>actors</th>\n",
       "      <th>gender</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>16</td>\n",
       "      <td>female</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fear</td>\n",
       "      <td>16</td>\n",
       "      <td>female</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fear</td>\n",
       "      <td>16</td>\n",
       "      <td>female</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angry</td>\n",
       "      <td>16</td>\n",
       "      <td>female</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>disgust</td>\n",
       "      <td>16</td>\n",
       "      <td>female</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2869</th>\n",
       "      <td>happy</td>\n",
       "      <td>8</td>\n",
       "      <td>female</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2871</th>\n",
       "      <td>happy</td>\n",
       "      <td>8</td>\n",
       "      <td>female</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2872</th>\n",
       "      <td>neutral</td>\n",
       "      <td>8</td>\n",
       "      <td>female</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2873</th>\n",
       "      <td>neutral</td>\n",
       "      <td>8</td>\n",
       "      <td>female</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2874</th>\n",
       "      <td>neutral</td>\n",
       "      <td>8</td>\n",
       "      <td>female</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1440 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     emotion_label actors  gender  \\\n",
       "0            angry     16  female   \n",
       "1             fear     16  female   \n",
       "2             fear     16  female   \n",
       "3            angry     16  female   \n",
       "5          disgust     16  female   \n",
       "...            ...    ...     ...   \n",
       "2869         happy      8  female   \n",
       "2871         happy      8  female   \n",
       "2872       neutral      8  female   \n",
       "2873       neutral      8  female   \n",
       "2874       neutral      8  female   \n",
       "\n",
       "                                                   path  \n",
       "0     /Users/helemanc/Documents/MasterAI/THESIS/Data...  \n",
       "1     /Users/helemanc/Documents/MasterAI/THESIS/Data...  \n",
       "2     /Users/helemanc/Documents/MasterAI/THESIS/Data...  \n",
       "3     /Users/helemanc/Documents/MasterAI/THESIS/Data...  \n",
       "5     /Users/helemanc/Documents/MasterAI/THESIS/Data...  \n",
       "...                                                 ...  \n",
       "2869  /Users/helemanc/Documents/MasterAI/THESIS/Data...  \n",
       "2871  /Users/helemanc/Documents/MasterAI/THESIS/Data...  \n",
       "2872  /Users/helemanc/Documents/MasterAI/THESIS/Data...  \n",
       "2873  /Users/helemanc/Documents/MasterAI/THESIS/Data...  \n",
       "2874  /Users/helemanc/Documents/MasterAI/THESIS/Data...  \n",
       "\n",
       "[1440 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAV_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RAV_train = []\n",
    "RAV_val = []\n",
    "RAV_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 120, 120)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in RAV_df.iterrows():\n",
    "    if row['actors'] in range(1,21): \n",
    "        RAV_train.append(row) \n",
    "    elif row['actors'] in range(21,23): \n",
    "        RAV_val.append(row)\n",
    "    elif row['actors'] in range(23,25): \n",
    "        RAV_test.append(row)\n",
    "len(RAV_train), len(RAV_val), len(RAV_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAV_train = pd.DataFrame(RAV_train)\n",
    "RAV_val = pd.DataFrame(RAV_val)\n",
    "RAV_test = pd.DataFrame(RAV_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RAV_train = RAV_train.drop(['actors'], 1)\n",
    "RAV_val = RAV_val.drop(['actors'], 1)\n",
    "RAV_test = RAV_test.drop(['actors'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAV_train.reset_index(drop=True, inplace = True) \n",
    "RAV_val.reset_index(drop=True, inplace = True) \n",
    "RAV_test.reset_index(drop=True, inplace = True ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SAVEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     120\n",
       "surprise     60\n",
       "disgust      60\n",
       "happy        60\n",
       "fear         60\n",
       "sadness      60\n",
       "angry        60\n",
       "Name: emotion_label, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the data location for SAVEE\n",
    "dir_list = os.listdir(SAVEE)\n",
    "\n",
    "# parse the filename to get the emotions\n",
    "emotion=[]\n",
    "path = []\n",
    "actors = []\n",
    "gender = []\n",
    "for i in dir_list:\n",
    "    actors.append(i[:2])\n",
    "    if i[-8:-6]=='_a':\n",
    "        emotion.append('angry')\n",
    "        gender.append('male')\n",
    "    elif i[-8:-6]=='_d':\n",
    "        emotion.append('disgust')\n",
    "        gender.append('male')\n",
    "    elif i[-8:-6]=='_f':\n",
    "        emotion.append('fear')\n",
    "        gender.append('male')\n",
    "    elif i[-8:-6]=='_h':\n",
    "        emotion.append('happy')\n",
    "        gender.append('male')\n",
    "    elif i[-8:-6]=='_n':\n",
    "        emotion.append('neutral')\n",
    "        gender.append('male')\n",
    "    elif i[-8:-6]=='sa':\n",
    "        emotion.append('sadness')\n",
    "        gender.append('male')\n",
    "    elif i[-8:-6]=='su':\n",
    "        emotion.append('surprise')\n",
    "        gender.append('male') \n",
    "    else:\n",
    "        emotion.append('Unknown') \n",
    "    path.append(SAVEE + i)\n",
    "    \n",
    "# Now check out the label count distribution \n",
    "SAVEE_df = pd.DataFrame(emotion, columns = ['emotion_label'])\n",
    "                      \n",
    "SAVEE_df = pd.concat([SAVEE_df,\n",
    "                      pd.DataFrame(actors, columns = ['actors']),\n",
    "                      pd.DataFrame(gender, columns = ['gender']), \n",
    "                      pd.DataFrame(path, columns = ['path'])], axis = 1)\n",
    "SAVEE_df.emotion_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>actors</th>\n",
       "      <th>gender</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sadness</td>\n",
       "      <td>JK</td>\n",
       "      <td>male</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>JK</td>\n",
       "      <td>male</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>DC</td>\n",
       "      <td>male</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>surprise</td>\n",
       "      <td>DC</td>\n",
       "      <td>male</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>DC</td>\n",
       "      <td>male</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emotion_label actors gender  \\\n",
       "0       sadness     JK   male   \n",
       "1       sadness     JK   male   \n",
       "2       neutral     DC   male   \n",
       "3      surprise     DC   male   \n",
       "4       neutral     DC   male   \n",
       "\n",
       "                                                path  \n",
       "0  /Users/helemanc/Documents/MasterAI/THESIS/Data...  \n",
       "1  /Users/helemanc/Documents/MasterAI/THESIS/Data...  \n",
       "2  /Users/helemanc/Documents/MasterAI/THESIS/Data...  \n",
       "3  /Users/helemanc/Documents/MasterAI/THESIS/Data...  \n",
       "4  /Users/helemanc/Documents/MasterAI/THESIS/Data...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVEE_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEE_train = []\n",
    "SAVEE_val = []\n",
    "SAVEE_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 120, 120)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DC, JE, JK, KL\n",
    "for index, row in SAVEE_df.iterrows(): \n",
    "    if row['actors'] == 'DC' or row ['actors'] == 'JE':\n",
    "        SAVEE_train.append(row)\n",
    "    elif row['actors'] == 'JK': \n",
    "        SAVEE_val.append(row)\n",
    "    else: \n",
    "        SAVEE_test.append(row)\n",
    "len(SAVEE_train), len(SAVEE_val), len(SAVEE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEE_train = pd.DataFrame(SAVEE_train)\n",
    "SAVEE_val = pd.DataFrame(SAVEE_val)\n",
    "SAVEE_test = pd.DataFrame(SAVEE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEE_train = SAVEE_train.drop(['actors'], 1)\n",
    "SAVEE_val = SAVEE_val.drop(['actors'], 1)\n",
    "SAVEE_test = SAVEE_test.drop(['actors'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAVEE_train = SAVEE_train.reset_index(drop=True) \n",
    "SAVEE_val = SAVEE_val.reset_index(drop=True) \n",
    "SAVEE_test = SAVEE_test.reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "angry       1200\n",
       "surprise     800\n",
       "fear         800\n",
       "disgust      800\n",
       "sadness      800\n",
       "neutral      800\n",
       "happy        400\n",
       "Name: emotion_label, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_list = os.listdir(TESS)\n",
    "dir_list.sort()\n",
    "dir_list\n",
    "\n",
    "path = []\n",
    "emotion = []\n",
    "gender = []\n",
    "actors = []\n",
    "\n",
    "for i in dir_list:\n",
    "    fname = os.listdir(TESS + i)\n",
    "    for f in fname:\n",
    "        if i == 'OAF_angry':\n",
    "            emotion.append('angry')\n",
    "            gender.append('female')\n",
    "            actors.append('OAF')\n",
    "        elif i == 'YAF_angry': \n",
    "            emotion.append('angry')\n",
    "            gender.append('female')\n",
    "            actors.append('YAF')\n",
    "            \n",
    "            \n",
    "        elif i == 'OAF_disgust' :\n",
    "            emotion.append('disgust')\n",
    "            gender.append('female')\n",
    "            actors.append('OAF')\n",
    "        elif i == 'YAF_disgust': \n",
    "            emotion.append('disgust')\n",
    "            gender.append('female')\n",
    "            actors.append('YAF')\n",
    "            \n",
    "            \n",
    "        elif i == 'OAF_Fear':\n",
    "            emotion.append('fear')\n",
    "            gender.append('female')\n",
    "            actors.append('OAF')\n",
    "        elif i == 'YAF_fear': \n",
    "            emotion.append('fear')\n",
    "            gender.append('female')\n",
    "            actors.append('YAF') \n",
    "            \n",
    "            \n",
    "        elif i == 'OAF_happy' :\n",
    "            emotion.append('happy')\n",
    "            gender.append('female')\n",
    "            actors.append('OAF')\n",
    "        elif i == 'YAF_happy': \n",
    "            emotion.append('angry')\n",
    "            gender.append('female')\n",
    "            actors.append('YAF')            \n",
    "            \n",
    "        elif i == 'OAF_neutral':\n",
    "            emotion.append('neutral')\n",
    "            gender.append('female')\n",
    "            actors.append('OAF')   \n",
    "        elif i == 'YAF_neutral': \n",
    "            emotion.append('neutral')\n",
    "            gender.append('female')\n",
    "            actors.append('YAF')      \n",
    "            \n",
    "                \n",
    "        elif i == 'OAF_Pleasant_surprise':\n",
    "            emotion.append('surprise')\n",
    "            gender.append('female')\n",
    "            actors.append('OAF')\n",
    "        \n",
    "        elif i == 'YAF_pleasant_surprised': \n",
    "            emotion.append('surprise')\n",
    "            gender.append('female')\n",
    "            actors.append('YAF')            \n",
    "            \n",
    "        elif i == 'OAF_Sad':\n",
    "            emotion.append('sadness')\n",
    "            gender.append('female')\n",
    "            actors.append('OAF')\n",
    "        elif i == 'YAF_sad': \n",
    "            emotion.append('sadness')\n",
    "            gender.append('female')\n",
    "            actors.append('YAF')            \n",
    "        else:\n",
    "            emotion.append('Unknown')\n",
    "        path.append(TESS + i + \"/\" + f)\n",
    "\n",
    "TESS_df = pd.DataFrame(emotion, columns = ['emotion_label'])\n",
    "TESS_df = pd.concat([TESS_df, pd.DataFrame(gender, columns = ['gender']), \n",
    "                     pd.DataFrame(actors, columns= ['actors']),\n",
    "                     pd.DataFrame(path, columns = ['path'])],axis=1)\n",
    "TESS_df.emotion_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TESS_df= TESS_df[~TESS_df.path.str.contains('noise')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESS_train = []\n",
    "TESS_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 1400)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in TESS_df.iterrows(): \n",
    "    if row['actors'] == 'YAF': \n",
    "        TESS_train.append(row)\n",
    "    else: \n",
    "        TESS_test.append(row)\n",
    "len(TESS_train), len(TESS_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESS_train = pd.DataFrame(TESS_train)\n",
    "TESS_test = pd.DataFrame(TESS_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESS_train = TESS_train.drop(['actors'], 1)\n",
    "TESS_test = TESS_test.drop(['actors'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESS_train = TESS_train.reset_index(drop=True) \n",
    "TESS_test  = TESS_test.reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CREMA-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "males = [1,\n",
    "5,\n",
    "11,\n",
    "14,\n",
    "15,\n",
    "16,\n",
    "17,\n",
    "19,\n",
    "22,\n",
    "23,\n",
    "26,\n",
    "27,\n",
    "31,\n",
    "32,\n",
    "33,\n",
    "34,\n",
    "35,\n",
    "36,\n",
    "38,\n",
    "39,\n",
    "41,\n",
    "42,\n",
    "44,\n",
    "45,\n",
    "48,\n",
    "50,\n",
    "51,\n",
    "57,\n",
    "59, \n",
    "62, \n",
    "64,\n",
    "65, \n",
    "66,\n",
    "67,\n",
    "68,\n",
    "69,\n",
    "70,\n",
    "71,\n",
    "77, \n",
    "80, \n",
    "81, \n",
    "83, \n",
    "85, \n",
    "86, \n",
    "87,\n",
    "88, \n",
    "90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "females = [ 2,\n",
    "3,\n",
    "4,\n",
    "6,\n",
    "7,\n",
    "8,\n",
    "9,\n",
    "10,\n",
    "12,\n",
    "13,\n",
    "18,\n",
    "20,\n",
    "21,\n",
    "24,\n",
    "25,\n",
    "28,\n",
    "29,\n",
    "30,\n",
    "37,\n",
    "40,\n",
    "43,\n",
    "46,\n",
    "47,\n",
    "49,\n",
    "52,\n",
    "53,\n",
    "54,\n",
    "55,\n",
    "56, \n",
    "58, \n",
    "60,\n",
    "61,\n",
    "63,\n",
    "72, \n",
    "73, \n",
    "74, \n",
    "75, \n",
    "76, \n",
    "78, \n",
    "79, \n",
    "82, \n",
    "84, \n",
    "89, \n",
    "91]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>actors</th>\n",
       "      <th>gender</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sadness</td>\n",
       "      <td>28</td>\n",
       "      <td>female</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angry</td>\n",
       "      <td>48</td>\n",
       "      <td>male</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disgust</td>\n",
       "      <td>27</td>\n",
       "      <td>male</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disgust</td>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happy</td>\n",
       "      <td>80</td>\n",
       "      <td>male</td>\n",
       "      <td>/Users/helemanc/Documents/MasterAI/THESIS/Data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emotion_label actors  gender  \\\n",
       "0       sadness     28  female   \n",
       "1         angry     48    male   \n",
       "2       disgust     27    male   \n",
       "3       disgust     32    male   \n",
       "4         happy     80    male   \n",
       "\n",
       "                                                path  \n",
       "0  /Users/helemanc/Documents/MasterAI/THESIS/Data...  \n",
       "1  /Users/helemanc/Documents/MasterAI/THESIS/Data...  \n",
       "2  /Users/helemanc/Documents/MasterAI/THESIS/Data...  \n",
       "3  /Users/helemanc/Documents/MasterAI/THESIS/Data...  \n",
       "4  /Users/helemanc/Documents/MasterAI/THESIS/Data...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crema_directory_list = os.listdir(CREMA)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "actors = []\n",
    "gender = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for file in crema_directory_list:\n",
    "\n",
    "    # storing file emotions\n",
    "    part=file.split('_')\n",
    "    \n",
    "    # use only high intensity files\n",
    "    if \"HI\" in part[3] :\n",
    "        actor = part[0][2:]\n",
    "        actors.append(actor)\n",
    "        if int(actor) in males:\n",
    "            gender.append('male')\n",
    "        else: \n",
    "            gender.append('female')\n",
    "    \n",
    "        # storing file paths\n",
    "        file_path.append(CREMA + file)\n",
    "        if part[2] == 'SAD':\n",
    "            file_emotion.append('sadness')\n",
    "        elif part[2] == 'ANG':\n",
    "            file_emotion.append('angry')\n",
    "        elif part[2] == 'DIS':\n",
    "            file_emotion.append('disgust')\n",
    "        elif part[2] == 'FEA':\n",
    "            file_emotion.append('fear')\n",
    "        elif part[2] == 'HAP':\n",
    "            file_emotion.append('happy')\n",
    "        elif part[2] == 'NEU':\n",
    "            file_emotion.append('neutral')\n",
    "        else:\n",
    "            file_emotion.append('Unknown')\n",
    "\n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['emotion_label'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['path'])\n",
    "actors_df = pd.DataFrame(actors, columns=['actors'])\n",
    "gender_df = pd.DataFrame(gender, columns=['gender'])                      \n",
    "Crema_df = pd.concat([emotion_df, actors_df, gender_df, path_df], axis=1)\n",
    "Crema_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 4)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Crema_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_files = {}\n",
    "\n",
    "for index, row in Crema_df.iterrows():\n",
    "    actor = row['actors']\n",
    "    if actor not in actor_files.keys(): \n",
    "        actor_files[actor] = 1\n",
    "    else: \n",
    "        actor_files[actor]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'28': 5,\n",
       " '48': 5,\n",
       " '27': 5,\n",
       " '32': 5,\n",
       " '80': 5,\n",
       " '59': 5,\n",
       " '39': 5,\n",
       " '43': 5,\n",
       " '56': 5,\n",
       " '31': 5,\n",
       " '24': 5,\n",
       " '51': 5,\n",
       " '44': 5,\n",
       " '77': 5,\n",
       " '62': 5,\n",
       " '88': 5,\n",
       " '55': 5,\n",
       " '40': 5,\n",
       " '13': 5,\n",
       " '06': 5,\n",
       " '35': 5,\n",
       " '20': 5,\n",
       " '87': 5,\n",
       " '09': 5,\n",
       " '78': 5,\n",
       " '65': 5,\n",
       " '70': 5,\n",
       " '23': 5,\n",
       " '36': 5,\n",
       " '01': 5,\n",
       " '14': 5,\n",
       " '47': 5,\n",
       " '52': 5,\n",
       " '58': 5,\n",
       " '37': 5,\n",
       " '22': 5,\n",
       " '71': 5,\n",
       " '64': 5,\n",
       " '50': 5,\n",
       " '45': 5,\n",
       " '57': 5,\n",
       " '42': 5,\n",
       " '29': 5,\n",
       " '53': 5,\n",
       " '46': 5,\n",
       " '33': 5,\n",
       " '26': 5,\n",
       " '34': 5,\n",
       " '21': 5,\n",
       " '15': 5,\n",
       " '08': 5,\n",
       " '86': 5,\n",
       " '79': 5,\n",
       " '63': 5,\n",
       " '76': 5,\n",
       " '25': 5,\n",
       " '30': 5,\n",
       " '07': 5,\n",
       " '12': 5,\n",
       " '41': 5,\n",
       " '54': 5,\n",
       " '89': 5,\n",
       " '81': 5,\n",
       " '49': 5,\n",
       " '38': 5,\n",
       " '05': 5,\n",
       " '10': 5,\n",
       " '61': 5,\n",
       " '74': 5,\n",
       " '69': 5,\n",
       " '18': 5,\n",
       " '83': 5,\n",
       " '17': 5,\n",
       " '02': 5,\n",
       " '73': 5,\n",
       " '66': 5,\n",
       " '84': 5,\n",
       " '91': 5,\n",
       " '90': 5,\n",
       " '85': 5,\n",
       " '03': 5,\n",
       " '16': 5,\n",
       " '67': 5,\n",
       " '72': 5,\n",
       " '68': 5,\n",
       " '82': 5,\n",
       " '19': 5,\n",
       " '11': 5,\n",
       " '04': 5,\n",
       " '75': 5,\n",
       " '60': 5}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_males = 0 \n",
    "count_females = 0 \n",
    "male_list = []\n",
    "for index, row in Crema_df.iterrows(): \n",
    "    gender = row['gender']\n",
    "    actor = row['actors']\n",
    "    if gender == 'male':\n",
    "        count_males +=1\n",
    "        if actor not in male_list: \n",
    "            male_list.append(actor)\n",
    "    else: \n",
    "        count_females +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 220)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_males, count_females"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are more males than females we will remove randomly 3 male actors (since there are exactly 5 audio files per actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "'''\n",
    "random.seed(42)\n",
    "males_to_remove = random.sample(male_list, 3)\n",
    "males_to_remove\n",
    "'''\n",
    "males_to_remove = ['17', '80', '88']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = []\n",
    "for index, row in Crema_df.iterrows(): \n",
    "    if row['actors'] not in males_to_remove: \n",
    "        new_df.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREMA_df = pd.DataFrame(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in CREMA_df.iterrows(): \n",
    "    if row['actors'] == '17': \n",
    "        print(\"Elements not removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_males = 0 \n",
    "count_females = 0 \n",
    "male_list = []\n",
    "female_list = []\n",
    "for index, row in CREMA_df.iterrows(): \n",
    "    gender = row['gender']\n",
    "    actor = row['actors']\n",
    "    if gender == 'male':\n",
    "        count_males +=1\n",
    "        if actor not in male_list: \n",
    "            male_list.append(actor)\n",
    "    else: \n",
    "        count_females +=1\n",
    "        if actor not in female_list: \n",
    "            female_list.append(actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 220)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_males, count_females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(female_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(male_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREMA_train = []\n",
    "CREMA_val = []\n",
    "CREMA_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "females_train = random.sample(female_list, 32)\n",
    "males_train = random.sample(male_list, 32)\n",
    "\n",
    "# remove the elements assigned to train \n",
    "for element in females_train:\n",
    "    if element in female_list:\n",
    "        female_list.remove(element)\n",
    "        \n",
    "for element in males_train:\n",
    "    if element in male_list:\n",
    "        male_list.remove(element)\n",
    "\n",
    "         \n",
    "females_val = random.sample(female_list, 6) \n",
    "males_val = random.sample(male_list, 6) \n",
    "\n",
    "# remove the elements assigned to val\n",
    "for element in females_val:\n",
    "    if element in female_list:\n",
    "        female_list.remove(element)\n",
    "        \n",
    "for element in males_val:\n",
    "    if element in male_list:\n",
    "        male_list.remove(element)\n",
    "        \n",
    "females_test = random.sample(female_list, 6) \n",
    "males_test = random.sample(male_list, 6)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['58',\n",
       "  '30',\n",
       "  '13',\n",
       "  '53',\n",
       "  '89',\n",
       "  '72',\n",
       "  '79',\n",
       "  '55',\n",
       "  '09',\n",
       "  '47',\n",
       "  '07',\n",
       "  '18',\n",
       "  '63',\n",
       "  '40',\n",
       "  '56',\n",
       "  '46',\n",
       "  '75',\n",
       "  '12',\n",
       "  '25',\n",
       "  '78',\n",
       "  '29',\n",
       "  '10',\n",
       "  '02',\n",
       "  '61',\n",
       "  '54',\n",
       "  '74',\n",
       "  '20',\n",
       "  '60',\n",
       "  '49',\n",
       "  '24',\n",
       "  '03',\n",
       "  '21'],\n",
       " ['39',\n",
       "  '51',\n",
       "  '57',\n",
       "  '19',\n",
       "  '44',\n",
       "  '81',\n",
       "  '45',\n",
       "  '32',\n",
       "  '87',\n",
       "  '27',\n",
       "  '01',\n",
       "  '35',\n",
       "  '31',\n",
       "  '66',\n",
       "  '50',\n",
       "  '33',\n",
       "  '41',\n",
       "  '11',\n",
       "  '68',\n",
       "  '16',\n",
       "  '38',\n",
       "  '62',\n",
       "  '71',\n",
       "  '83',\n",
       "  '48',\n",
       "  '65',\n",
       "  '22',\n",
       "  '64',\n",
       "  '85',\n",
       "  '26',\n",
       "  '34',\n",
       "  '14'],\n",
       " ['04', '82', '84', '08', '37', '52'],\n",
       " ['15', '05', '77', '42', '23', '69'],\n",
       " ['43', '73', '91', '28', '76', '06'],\n",
       " ['67', '90', '70', '59', '86', '36'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "females_train, males_train, females_val, males_val, females_test, males_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = females_train + males_train \n",
    "val = females_val + males_val \n",
    "test = females_test + males_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index, row in CREMA_df.iterrows(): \n",
    "    gender = row['gender']\n",
    "    actor = row['actors']\n",
    "    if actor in train: \n",
    "        CREMA_train.append(row)\n",
    "    elif actor in val: \n",
    "        CREMA_val.append(row)\n",
    "    else:\n",
    "        CREMA_test.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREMA_train = pd.DataFrame(CREMA_train) \n",
    "CREMA_val = pd.DataFrame(CREMA_val) \n",
    "CREMA_test = pd.DataFrame(CREMA_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((320, 4), (60, 4), (60, 4))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CREMA_train.shape, CREMA_val.shape, CREMA_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREMA_train = CREMA_train.drop(['actors'], 1)\n",
    "CREMA_val = CREMA_val.drop(['actors'], 1)\n",
    "CREMA_test = CREMA_test.drop(['actors'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CREMA_train = CREMA_train.reset_index(drop=True) \n",
    "CREMA_val = CREMA_val.reset_index(drop = True) \n",
    "CREMA_test = CREMA_test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise(samples):\n",
    "    \"\"\"\n",
    "    :param samples: an array representing the sampled audio file\n",
    "    :type samples: float\n",
    "    :return: an array representing the clean audio file\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    samples_wiener = scipy.signal.wiener(samples)\n",
    "    return samples_wiener\n",
    "\n",
    "def resample(input_data, sample_rate, required_sample_rate=16000, amplify=False):\n",
    "    \"\"\"\n",
    "    Resampling function. Takes an audio with sample_rate and resamples\n",
    "    it to required_sample_rate. Optionally, amplifies volume.\n",
    "    ​\n",
    "    :param input_data: Input audio data\n",
    "    :type input_data: numpy.ndarray\n",
    "    :param sample_rate: Sample rate of input audio data\n",
    "    :type sample_rate: int\n",
    "    :param required_sample_rate: Sample rate to resample original audio, defaults to 16000\n",
    "    :type required_sample_rate: int, optional\n",
    "    :param amplify: Whether to amplify audio volume, defaults to False\n",
    "    :type amplify: bool, optional\n",
    "    :return: Resampled audio and new sample rate\n",
    "    :rtype: numpy.ndarray, int\n",
    "    \"\"\"\n",
    "    if sample_rate < required_sample_rate:\n",
    "        resampling_factor = int(round(required_sample_rate/sample_rate, 0))\n",
    "        new_rate = sample_rate * resampling_factor\n",
    "        samples = len(input_data) * resampling_factor\n",
    "        resampled = signal.resample(input_data, samples)\n",
    "    elif sample_rate > required_sample_rate:\n",
    "        resampling_factor = int(round(sample_rate/required_sample_rate, 0))\n",
    "        new_rate = int(sample_rate / resampling_factor)\n",
    "        resampled = signal.decimate(input_data, resampling_factor)\n",
    "    else:\n",
    "        resampling_factor = 1\n",
    "        new_rate = sample_rate\n",
    "        resampled = input_data\n",
    "    if amplify and input_data.size > 0:\n",
    "        absolute_values = np.absolute(resampled)\n",
    "        max_value = np.amax(absolute_values)\n",
    "        max_range = np.iinfo(np.int16).max\n",
    "        amplify_factor = max_range/max_value\n",
    "        resampled = resampled * amplify_factor\n",
    "        resampled = resampled.round()\n",
    "    return resampled.astype(np.float32), new_rate\n",
    "\n",
    "\n",
    "def make_predictions(dataset, labels, model_name): \n",
    "    predictions = []\n",
    "    model_predictions_list = []\n",
    "    counter = 0\n",
    "    for filepath in tqdm(dataset['path']):\n",
    "        samples, sample_rate = fe.read_file(filepath)\n",
    "        samples, sample_rate = resample(samples, sample_rate)\n",
    "        new_samples = fe.cut_pad(samples)\n",
    "        #new_filepath = \"tmp.wav\"\n",
    "        for dirpath, dirnames, filenames in os.walk(MODELS_FOLDER):\n",
    "            # iterate over the list of dirnames to load the convmodel\n",
    "            # iterate over the list of filenames to get the svm model\n",
    "            for model in dirnames:\n",
    "                model_type = 'conv'\n",
    "                parts = model.split('_')\n",
    "                num_exp = parts[1]\n",
    "                num_data = parts[2].split('.')[0]\n",
    "                id_exp = num_exp + '_' + num_data\n",
    "                if id_exp == model_name: \n",
    "                    print(id_exp)\n",
    "                    model_path = os.path.join(dirpath, model)\n",
    "                    conv_model = tf.keras.models.load_model(model_path)\n",
    "                    for scaler in os.listdir(SCALERS_FOLDER):\n",
    "                        parts = scaler.split('_')\n",
    "                        num_exp = parts[1]\n",
    "                        num_data = parts[2].split('.')[0]\n",
    "                        id_exp_scaler = num_exp + '_' + num_data\n",
    "                        if id_exp == id_exp_scaler:\n",
    "                            # print(\"Loading scaler: \", scaler)\n",
    "                            scaler_conv_model = scaler\n",
    "                            mfccs = fe.mfccs_scaled(new_samples, scaler_conv_model, id_exp)\n",
    "                            pred = ep.make_predictions(conv_model, model_type, mfccs, 'majority')\n",
    "                            # add result to list and dictionary\n",
    "                            predictions.append(pred)\n",
    "                            #model_predictions[id_exp] = pred\n",
    "            break\n",
    "        #final_prediction, model_predictions = ensemble.ensemble(new_samples, prediction_scheme, return_model_predictions = True)\n",
    "\n",
    "        #predictions.append(final_prediction)\n",
    "        #model_predictions_list.append(model_predictions) \n",
    "        print(\"True label\", labels[counter], \"Predicted label\", predictions[counter])\n",
    "        counter+=1\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe_predictions(prediction_list):\n",
    "    df_predictions = pd.DataFrame(prediction_list)\n",
    "    return df_predictions\n",
    "\n",
    "def create_dataframe_res(labels, df_predictions, dataset): \n",
    "    df_res = pd.concat([labels, \n",
    "                    df_predictions, \n",
    "                    dataset.path], axis = 1, ignore_index=True, sort=False)\n",
    "    new_header = []\n",
    "    new_header.append('true_label')\n",
    "    new_header.append('pred_label')\n",
    "    new_header = new_header \n",
    "    new_header.append('path')\n",
    "    df_res.columns = new_header\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set RAVDESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = RAV_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.random.randint(2, size=df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_enc = {'fear':1, 'disgust':1, 'neutral':0, 'calm':0,  'happy':0, 'sadness':1, 'surprise':0, 'angry':1}\n",
    "labels= pd.Series(list(df_test.emotion_label)).replace(emotion_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = create_dataframe_predictions(predictions)\n",
    "df_res = create_dataframe_res(labels, df_predictions, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.52      0.50        56\n",
      "           1       0.55      0.52      0.53        64\n",
      "\n",
      "    accuracy                           0.52       120\n",
      "   macro avg       0.52      0.52      0.52       120\n",
      "weighted avg       0.52      0.52      0.52       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_res.true_label, df_res.pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set SAVEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = SAVEE_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.random.randint(2, size=df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_enc = {'fear':1, 'disgust':1, 'neutral':0, 'calm':0,  'happy':0, 'sadness':1, 'surprise':0, 'angry':1}\n",
    "labels= pd.Series(list(df_test.emotion_label)).replace(emotion_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = create_dataframe_predictions(predictions)\n",
    "df_res = create_dataframe_res(labels, df_predictions, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.55      0.53        60\n",
      "           1       0.51      0.47      0.49        60\n",
      "\n",
      "    accuracy                           0.51       120\n",
      "   macro avg       0.51      0.51      0.51       120\n",
      "weighted avg       0.51      0.51      0.51       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_res.true_label, df_res.pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set TESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = TESS_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.random.randint(2, size=df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_enc = {'fear':1, 'disgust':1, 'neutral':0, 'calm':0,  'happy':0, 'sadness':1, 'surprise':0, 'angry':1}\n",
    "labels= pd.Series(list(df_test.emotion_label)).replace(emotion_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = create_dataframe_predictions(predictions)\n",
    "df_res = create_dataframe_res(labels, df_predictions, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.50      0.46       600\n",
      "           1       0.57      0.50      0.53       800\n",
      "\n",
      "    accuracy                           0.50      1400\n",
      "   macro avg       0.50      0.50      0.50      1400\n",
      "weighted avg       0.51      0.50      0.50      1400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_res.true_label, df_res.pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set CREMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = CREMA_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.random.randint(2, size=df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_enc = {'fear':1, 'disgust':1, 'neutral':0, 'calm':0,  'happy':0, 'sadness':1, 'surprise':0, 'angry':1}\n",
    "labels= pd.Series(list(df_test.emotion_label)).replace(emotion_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = create_dataframe_predictions(predictions)\n",
    "df_res = create_dataframe_res(labels, df_predictions, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.67      0.39        12\n",
      "           1       0.87      0.56      0.68        48\n",
      "\n",
      "    accuracy                           0.58        60\n",
      "   macro avg       0.57      0.61      0.54        60\n",
      "weighted avg       0.75      0.58      0.62        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_res.true_label, df_res.pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set RTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([RAV_test, SAVEE_test, TESS_test])\n",
    "df_test.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.random.randint(2, size=df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_enc = {'fear':1, 'disgust':1, 'neutral':0, 'calm':0,  'happy':0, 'sadness':1, 'surprise':0, 'angry':1}\n",
    "labels= pd.Series(list(df_test.emotion_label)).replace(emotion_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = create_dataframe_predictions(predictions)\n",
    "df_res = create_dataframe_res(labels, df_predictions, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.49      0.46       716\n",
      "           1       0.56      0.50      0.53       924\n",
      "\n",
      "    accuracy                           0.50      1640\n",
      "   macro avg       0.50      0.50      0.49      1640\n",
      "weighted avg       0.50      0.50      0.50      1640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_res.true_label, df_res.pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set RTSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([RAV_test, SAVEE_test, TESS_test, CREMA_test])\n",
    "df_test.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.random.randint(2, size=df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_enc = {'fear':1, 'disgust':1, 'neutral':0, 'calm':0,  'happy':0, 'sadness':1, 'surprise':0, 'angry':1}\n",
    "labels= pd.Series(list(df_test.emotion_label)).replace(emotion_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = create_dataframe_predictions(predictions)\n",
    "df_res = create_dataframe_res(labels, df_predictions, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.51      0.47       728\n",
      "           1       0.58      0.51      0.54       972\n",
      "\n",
      "    accuracy                           0.51      1700\n",
      "   macro avg       0.51      0.51      0.51      1700\n",
      "weighted avg       0.52      0.51      0.51      1700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_res.true_label, df_res.pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-Class Biased Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set RAVDESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = RAV_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.ones(df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1.])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_enc = {'fear':1, 'disgust':1, 'neutral':0, 'calm':0,  'happy':0, 'sadness':1, 'surprise':0, 'angry':1}\n",
    "labels= pd.Series(list(df_test.emotion_label)).replace(emotion_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = create_dataframe_predictions(predictions)\n",
    "df_res = create_dataframe_res(labels, df_predictions, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.53      1.00      0.70        64\n",
      "\n",
      "    accuracy                           0.53       120\n",
      "   macro avg       0.27      0.50      0.35       120\n",
      "weighted avg       0.28      0.53      0.37       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/helemanc/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/helemanc/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/helemanc/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_res.true_label, df_res.pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set SAVEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = SAVEE_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.ones(df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_enc = {'fear':1, 'disgust':1, 'neutral':0, 'calm':0,  'happy':0, 'sadness':1, 'surprise':0, 'angry':1}\n",
    "labels= pd.Series(list(df_test.emotion_label)).replace(emotion_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = create_dataframe_predictions(predictions)\n",
    "df_res = create_dataframe_res(labels, df_predictions, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        60\n",
      "           1       0.50      1.00      0.67        60\n",
      "\n",
      "    accuracy                           0.50       120\n",
      "   macro avg       0.25      0.50      0.33       120\n",
      "weighted avg       0.25      0.50      0.33       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/helemanc/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/helemanc/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/helemanc/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_res.true_label, df_res.pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set TESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = TESS_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.ones(df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_enc = {'fear':1, 'disgust':1, 'neutral':0, 'calm':0,  'happy':0, 'sadness':1, 'surprise':0, 'angry':1}\n",
    "labels= pd.Series(list(df_test.emotion_label)).replace(emotion_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = create_dataframe_predictions(predictions)\n",
    "df_res = create_dataframe_res(labels, df_predictions, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       600\n",
      "           1       0.57      1.00      0.73       800\n",
      "\n",
      "    accuracy                           0.57      1400\n",
      "   macro avg       0.29      0.50      0.36      1400\n",
      "weighted avg       0.33      0.57      0.42      1400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/helemanc/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/helemanc/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/helemanc/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_res.true_label, df_res.pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set CREMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = CREMA_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.ones(df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_enc = {'fear':1, 'disgust':1, 'neutral':0, 'calm':0,  'happy':0, 'sadness':1, 'surprise':0, 'angry':1}\n",
    "labels= pd.Series(list(df_test.emotion_label)).replace(emotion_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = create_dataframe_predictions(predictions)\n",
    "df_res = create_dataframe_res(labels, df_predictions, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.80      1.00      0.89        48\n",
      "\n",
      "    accuracy                           0.80        60\n",
      "   macro avg       0.40      0.50      0.44        60\n",
      "weighted avg       0.64      0.80      0.71        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/helemanc/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/helemanc/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/helemanc/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_res.true_label, df_res.pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set RTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([RAV_test, SAVEE_test, TESS_test])\n",
    "df_test.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.ones(df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_enc = {'fear':1, 'disgust':1, 'neutral':0, 'calm':0,  'happy':0, 'sadness':1, 'surprise':0, 'angry':1}\n",
    "labels= pd.Series(list(df_test.emotion_label)).replace(emotion_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = create_dataframe_predictions(predictions)\n",
    "df_res = create_dataframe_res(labels, df_predictions, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       716\n",
      "           1       0.56      1.00      0.72       924\n",
      "\n",
      "    accuracy                           0.56      1640\n",
      "   macro avg       0.28      0.50      0.36      1640\n",
      "weighted avg       0.32      0.56      0.41      1640\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/helemanc/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/helemanc/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/helemanc/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_res.true_label, df_res.pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set RTSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([RAV_test, SAVEE_test, TESS_test, CREMA_test])\n",
    "df_test.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.ones(df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_enc = {'fear':1, 'disgust':1, 'neutral':0, 'calm':0,  'happy':0, 'sadness':1, 'surprise':0, 'angry':1}\n",
    "labels= pd.Series(list(df_test.emotion_label)).replace(emotion_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = create_dataframe_predictions(predictions)\n",
    "df_res = create_dataframe_res(labels, df_predictions, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       728\n",
      "           1       0.57      1.00      0.73       972\n",
      "\n",
      "    accuracy                           0.57      1700\n",
      "   macro avg       0.29      0.50      0.36      1700\n",
      "weighted avg       0.33      0.57      0.42      1700\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/helemanc/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/helemanc/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/helemanc/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_res.true_label, df_res.pred_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
